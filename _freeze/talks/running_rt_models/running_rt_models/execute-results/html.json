{
  "hash": "81bb356f881240beb277f6d2449062e8",
  "result": {
    "markdown": "---\ntitle: Estimating variant growth rates from sample frequencies\nsubtitle: '`rt_from_frequency_dynamics` package overview'\nauthor: Marlin Figgins\ndate: 05-16-2022\nexecute:\n  eval: false\n  echo: true\n---\n\n## General overview\n\n:::: { .columns}\n::: {.column width=\"70%\"}\n\n### Main points of the package\n\n- Estimating variant growth rates and advantages from variant frequencies\n- Multiple options for models including observation likelihoods for both incidence data and sequences\n- Inference is handled behind the scenes by [Numpyro](https://num.pyro.ai/en/stable/)  \n:::\n\n::: {.column width=\"30%\"}\n:::\n\n::::\n\n## Data format\n\nThe package accepts two general formats for data files.\n\n:::: { .columns}\n::: {.column width=\"50%\"}\n### Case data\n\n```\ndate\tlocation\tcases\n2022-01-13\tArizona\t18573\n2022-01-14\tArizona\t20257\n2022-01-15\tArizona\t24962\n2022-01-16\tArizona\t21637\n2022-01-17\tArizona\t12066\n2022-01-18\tArizona\t23836\n2022-01-19\tArizona\t20497\n2022-01-20\tArizona\t17724\n... ... ...\n```\n\n:::\n\n::: {.column width=\"50%\"}\n### Sequence data\n\n```\ndate\tlocation\tvariant\tsequences\n2022-01-25\tArizona\tother\t1\n2022-02-28\tArizona\tother\t1\n2022-03-29\tArizona\tother\t1\n2022-04-04\tArizona\tother\t1\n2022-04-20\tArizona\tother\t1\n2022-01-13\tArizona\tDelta\t21\n2022-01-14\tArizona\tDelta\t12\n2022-01-15\tArizona\tDelta\t11\n... ... ... ...\n```\n:::\n\n\n<!-- Right now, these files are living in the [rt-from-frequency-dynamics repo](https://github.com/blab/rt-from-frequency-dynamics) -->\n::::\n\n\n## Loading the data to python is pretty easy \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\ngithub_prefix = \"https://raw.githubusercontent.com/blab/rt-from-frequency-dynamics/master/data/omicron-us-split/omicron-us-split\"\nraw_cases = pd.read_csv(github_prefix + \"_location-case-counts.tsv\", sep=\"\\t\")\nraw_seq = pd.read_csv(github_prefix + \"_location-variant-sequence-counts.tsv\", sep=\"\\t\")\n```\n:::\n\n\n## Data objects\n\nWithin the package, there are several classes which wrap and convert these files into something that numpyro can use.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Filtering to Washington state\nlocation = \"Washington\"\nraw_cases_loc = raw_cases[raw_cases.location == location]\nraw_seq_loc = raw_seq[raw_seq.location == location]\n\nvariant_data = rf.VariantData(raw_cases_loc, raw_seq_loc)\n```\n:::\n\n\n## Defining models\n\n- Models are also defined with classes and these classes hold the actual numpyro model.\n\n- We can define a multinomial logistic regression model as:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nmlr_model = rf.MultinomialLogisticRegression(gen)\n```\n:::\n\n\n- We can define a variant renewal model as:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nrenewal_model = rf.RenewalModel(gen, delays, RLik, CLik, SLik, v_names)\n```\n:::\n\n\n## Fitting models\n\n- Fitting models for a single location can be done with the `fit_SVI` and `fit_MCMC` models which return a posterior handler.\n- Posterior handlers hold the parameter estimates and the data.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nposterior_handler = rf.fit_SVI(variant_data, renewal_model, optimizer, iterations, num_samples)\n```\n:::\n\n\n- In reality, we'll often do inference on multiple data sets with the same specified model.\n- This can be done with `fit_SVI_locations` or `fit_MCMC_locations` and returns a multi-posterior handler.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nmulti_posteriors = rf.fit_SVI_locations(raw_cases, raw_seq, locations, renewal_model, optimizer, iterations, num_samples)\n```\n:::\n\n\n<!-- This could be changed to take in a vector of variant_data -->\n\n## Posterior results\n\n- The parameter estimates can be unpacked into posterior and data as:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nposterior, variant_data = rf.unpack_model(multi_posteriors, location)\n```\n:::\n\n\n- Here, `posterior` will be a dictionary with keys being the variable names and each entry being a array with size `(num_samples, var_shape...)`\n\n## Demo: An actual use-case\n\n- The best way to see how this works is to try it out.\n- Let's go over how I actually do these analyses using this package.\n\n## Using configs to specify analyses\n\n- There's a lot more options for model fitting to be configured here. \n- I've explored using a config file to specify how analyses ought to be run and which options to use.\n\n```{yaml}\ndata:\n  name: \"my_estimates\" # Model name\n  case_path: \"\"\n  seq_path: \"\"\n\nsettings:\n  fit: true # Fit the model?\n  save: true # Save model state?\n  load: false # Load old model?\n  export_tsv: true  # Export model results as tsv\n  export_path: \"../estimates/my_estimates\" # Where to put estimates\n  export_fig: false # Update figures? Not implemented yet.\n  ps: [0.5, 0.8, 0.95] # HPDI intervals to be exported\n\nmodel:\n  seed_L: 14\n  forecast_L: 7\n  R_likelihood: \"GARW\" # Options: GARW, Free, Fixed\n  C_likelihood: \"ZINegBinom\" # Options: NegBinom, Poisson, ZINegBinom, ZIPoisson\n  S_likelihood: \"DirMultinomial\" # Options: DirMultinomial and Multinomial\n  prior_case_dispersion: 0.01 # Ignored if using Poisson\n  prior_seq_dispersion: 100.0 # Ignored if using Multinomial\n  k: 7 # Number of spline knots to use\n  generation_time: # Specify mean and standard deviation for delay\n    mean: 3.2\n    sd: 1.2\n    family: \"Gamma\" # Options: Gamma and LogNormal\n  delays: # Specify the delays between infection and sampling\n    incub:\n      mean: 2.1\n      sd: 1.2\n      family: \"LogNormal\"\n\ninference:\n  iters: 100000 # Number of iterations for SVI\n  lr: 1e-3 # Learning rate for the model\n  num_samples: 1500\n```\n\n",
    "supporting": [
      "running_rt_models_files"
    ],
    "filters": [],
    "includes": {}
  }
}