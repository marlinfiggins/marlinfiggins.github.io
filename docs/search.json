[
  {
    "objectID": "writing/writing.html",
    "href": "writing/writing.html",
    "title": "Writing",
    "section": "",
    "text": "Over the years, I’ve written many poems and essays. Here are my publications chronologically. Please look out for my future work!\n\n2020\nFiction of the Wired: Loneliness and the Limits of the Internet. Smart Museum.\nHomebound: A Microchapbook. Kissing Dynamite Poetry Press.\n\n\n2019\n[ ] Approaches Me in a Coffee Shop Asking Why I Left. Kissing Dynamite Poetry.\nSelf-Portrait with Radio & Moon. Nostrovia Press.\nIf What I’m Told Is True… Frontier Poetry.\nMemphis is the Most Beautiful of Cities. Ghost City Review.\nLetters to Dark Boy: On Rejecting the Body, On the Remnants of Pride. Menacing Hedge.\n\n\n2018\nThe Lost Boy is Quenched. Glass: A Journal of Poetry.\nThe Poet As Glutton. Narrative Northeast.\nBlack As Thin Body. Cotton Xenomorph.\nThe Lost Boy is Romantic. The Shallow Ends.\n\n\nManuscripts in preparation\n\nDark Boy Remaining (poetry, formerly titled Prayers Before the Moon)\nDark Boy and Muse (poetry)\nSTEM and Blood (essays)"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#who-am-i",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#who-am-i",
    "title": "Epidemics, evolution, and too much math",
    "section": "Who am I?",
    "text": "Who am I?\n\nUChicago alumnus (BS, Mathematics 2019)\nCurrent PhD student in Applied Math at Fred Hutch and the University of Washington in the Bedford lab.\nNow, I study transmission and evolution of pathogens like influenza and SARS-CoV-2."
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#epidemics-are-often-observed-with-counts",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#epidemics-are-often-observed-with-counts",
    "title": "Epidemics, evolution, and too much math",
    "section": "Epidemics are often observed with counts",
    "text": "Epidemics are often observed with counts"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#we-use-r_t-to-infer-the-direction-of-epidemics",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#we-use-r_t-to-infer-the-direction-of-epidemics",
    "title": "Epidemics, evolution, and too much math",
    "section": "We use \\(R_{t}\\) to infer the direction of epidemics",
    "text": "We use \\(R_{t}\\) to infer the direction of epidemics\n{.fig-align=“center”}"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#pathogens-have-an-evolutionary-history",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#pathogens-have-an-evolutionary-history",
    "title": "Epidemics, evolution, and too much math",
    "section": "Pathogens have an evolutionary history",
    "text": "Pathogens have an evolutionary history"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#mutation-can-lead-to-transmission-differences",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#mutation-can-lead-to-transmission-differences",
    "title": "Epidemics, evolution, and too much math",
    "section": "Mutation can lead to transmission differences",
    "text": "Mutation can lead to transmission differences"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#variant-differences-may-not-show-in-cases",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#variant-differences-may-not-show-in-cases",
    "title": "Epidemics, evolution, and too much math",
    "section": "Variant differences may not show in cases",
    "text": "Variant differences may not show in cases"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#variant-frequencies-allow-us-to-monitor-change",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#variant-frequencies-allow-us-to-monitor-change",
    "title": "Epidemics, evolution, and too much math",
    "section": "Variant frequencies allow us to monitor change",
    "text": "Variant frequencies allow us to monitor change"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#turning-frequencies-into-growth-advantages",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#turning-frequencies-into-growth-advantages",
    "title": "Epidemics, evolution, and too much math",
    "section": "Turning frequencies into growth advantages",
    "text": "Turning frequencies into growth advantages"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#transmission-models-with-various-strains",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#transmission-models-with-various-strains",
    "title": "Epidemics, evolution, and too much math",
    "section": "Transmission models with various strains",
    "text": "Transmission models with various strains\n{.fig-align=“center”}"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#applying-this-to-sars-cov-2-and-covid-19-in-the-usa",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#applying-this-to-sars-cov-2-and-covid-19-in-the-usa",
    "title": "Epidemics, evolution, and too much math",
    "section": "Applying this to SARS-CoV-2 and COVID-19 in the USA",
    "text": "Applying this to SARS-CoV-2 and COVID-19 in the USA"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#applying-this-to-sars-cov-2-and-covid-19-in-the-usa-1",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#applying-this-to-sars-cov-2-and-covid-19-in-the-usa-1",
    "title": "Epidemics, evolution, and too much math",
    "section": "Applying this to SARS-CoV-2 and COVID-19 in the USA",
    "text": "Applying this to SARS-CoV-2 and COVID-19 in the USA"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#applying-this-to-sars-cov-2-and-covid-19-in-the-usa-2",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#applying-this-to-sars-cov-2-and-covid-19-in-the-usa-2",
    "title": "Epidemics, evolution, and too much math",
    "section": "Applying this to SARS-CoV-2 and COVID-19 in the USA",
    "text": "Applying this to SARS-CoV-2 and COVID-19 in the USA"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#developing-tools-for-analyzing-epidemics",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#developing-tools-for-analyzing-epidemics",
    "title": "Epidemics, evolution, and too much math",
    "section": "Developing tools for analyzing epidemics",
    "text": "Developing tools for analyzing epidemics\n\nHow did we go from a one-off model to a tool that we can continually use?\nMy research is focused on building tools and toolkits for analyzing evolution of pathogens.\nThis includes both software (like that used to make today’s figures) and mathematical machinery that teaches how and why growth advantages may appear between pathogen variants."
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#takeaways",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#takeaways",
    "title": "Epidemics, evolution, and too much math",
    "section": "Takeaways",
    "text": "Takeaways\n\nUnderstanding epidemics is complicated by feedback loop between evolution and transmission\nMathematics and statistics are tools for analyzing data that can provide new insight.\nOnce we have an idea in mind, we can build re-usable and customizable tools for analyzing new data to make new analyses easier.\nThis allows us to make many forecasts as we receive new data and opens the door for evolutionary forecasting"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#questions",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#questions",
    "title": "Epidemics, evolution, and too much math",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Methods for epidemic-evolutionary forecasting",
    "section": "",
    "text": "Population dynamics and evolution can occur on similar time scales when mutation occurs rapidly along replication. The case of transmission of infectious diseases is an especially interesting example of this.\nAs infected individuals transmit to one another, pathogens can accumulate mutations which can change the potential for escape from immune responses or increased transmissibility and can lead to differential fitness for different genetic variants.\n\n\n\n\nTransmission tree for a two variant system. Dashed lines denote new mutations.\n\n\n\n\n\n\n\n\nIn order to quantify the fitness of genetic variants at transmission, I developed a method from learning effective reproduction numbers and variant relative growth advantages using sequence and case count data.(Figgins and Bedford 2022). We apply this method to several US states to learn about the growth advantages for SARS-CoV-2 variants of concern.\n\n\n\nEstimating variant-specific effective reproductive numbers from case and sequence data."
  },
  {
    "objectID": "research.html#epidemic-evolutionary-dynamics",
    "href": "research.html#epidemic-evolutionary-dynamics",
    "title": "Methods for epidemic-evolutionary forecasting",
    "section": "",
    "text": "Population dynamics and evolution can occur on similar time scales when mutation occurs rapidly along replication. The case of transmission of infectious diseases is an especially interesting example of this.\nAs infected individuals transmit to one another, pathogens can accumulate mutations which can change the potential for escape from immune responses or increased transmissibility and can lead to differential fitness for different genetic variants.\n\n\n\n\nTransmission tree for a two variant system. Dashed lines denote new mutations.\n\n\n\n\n\n\n\n\nIn order to quantify the fitness of genetic variants at transmission, I developed a method from learning effective reproduction numbers and variant relative growth advantages using sequence and case count data.(Figgins and Bedford 2022). We apply this method to several US states to learn about the growth advantages for SARS-CoV-2 variants of concern.\n\n\n\nEstimating variant-specific effective reproductive numbers from case and sequence data."
  },
  {
    "objectID": "research.html#improving-evolutionary-forecasts",
    "href": "research.html#improving-evolutionary-forecasts",
    "title": "Methods for epidemic-evolutionary forecasting",
    "section": "Improving evolutionary forecasts",
    "text": "Improving evolutionary forecasts\n\nForecasting evolution can be useful for improving vaccination\nAs viruses mutate, vaccine effectiveness can wane due to immune mismatch between circulating virus strains and strains used in vaccines. For viruses like SARS-CoV-2 and influenza understanding how viruses differ in their fitness can help us to better forecast their evolution. This allows us better understand the process of evolution itself, but also has practical applications in improving vaccines.\nCurrently, I am tying in mechanistic models of transmission, immunity, and recovery to improve methods for evolutionary forecasting. Including more information about these processes allows us to get better estimates of relative fitness and better forecasts for virus populations.\n\n\nRobust evaluation of frequency forecasts\nAdditionally, we develop methods for evaluating models for the purpose of evolutionary forecasts.\nWe’ve developed pipelines for data curation, model evaluation and comparison to see how various forecast models compare on short-term forecasts to recreate and analyze the difficulties in real-time evolutionary forecasting. Through this work, we analyzed how the data collection process might affect our ability to forecast in the short and medium-term. (Figgins, Abousamra, and Bedford. 2023.)"
  },
  {
    "objectID": "research.html#tool-building-enables-others-to-learn-and-contribute",
    "href": "research.html#tool-building-enables-others-to-learn-and-contribute",
    "title": "Methods for epidemic-evolutionary forecasting",
    "section": "Tool-building enables others to learn and contribute",
    "text": "Tool-building enables others to learn and contribute\n\nAutomated SARS-CoV-2 Forecasts\nWith the team at Nextstrain, we’ve developed an open source public dashboard of interactive SARS-CoV-2 variant frequencies and forecasts globally: Nextstrain Forecasts.\n\n\n\nStatic image of Nextstrain Forecasts. Link to current dashboard.\n\n\nOur work is fully open source, and our results are publicly available and updated daily as new data comes in. We’ve collaborated with data providers worldwide and have developed robust methodologies for forecasting SARS-CoV-2 variant dynamics."
  },
  {
    "objectID": "research.html#tools-for-evolutionary-forecasts-of-variants",
    "href": "research.html#tools-for-evolutionary-forecasts-of-variants",
    "title": "Methods for epidemic-evolutionary forecasting",
    "section": "Tools for evolutionary forecasts of variants",
    "text": "Tools for evolutionary forecasts of variants\nI’ve also spearheaded the development of a Python package called evofr. Evofr is a production-level library specializing in implementing Bayesian models for evolutionary forecasting. Evofr is designed to provide a simple interface for quick model development and scalable inference on large genomic data sets (&gt; 2 million sequences) and time series while emphasizing ease of use and adoption by public health officials globally.\nBy design, evofr is modular allowing users to mix data types, models, and inference methods, allowing for backends in numpyro, pymc, custom posterior densities as well as variational inference, maximum likelihood estimation, and full Bayesian inference via MCMC.\nEvofr has found use is several projects in the lab such as Nextstrain Forecasts and several academic projects including Paredes et al. Cell, 2024., Figgins, Abousamra, and Bedford. 2023., Weil et al. 2022, and Weil et al. 2024.\n\nBuilding tools to make science more accessible and collaborative\nIn general, I am motivated by the desire to build tools to help folks answer questions they have about epidemics and how pathogens evolve. For this purpose, much of my time is dedicated to making sure the models and methods I develop are available freely and well documented, so that others can learn about them and make their own contributions to science.\n\n\nScience communication and education comes with the territory\nIn my opinion, science is the process of asking questions, answering questions, and presenting information. Further, I believe an important part of the process of doing science or really any kind of inquiry is enabling others to take action. To me, this primarily means creating resources for others to use whether that be educational material, a new software package, or any other tool."
  },
  {
    "objectID": "publications/publications.html",
    "href": "publications/publications.html",
    "title": "Publications",
    "section": "",
    "text": "SARS-CoV-2 diversity and transmission on a university campus across two academic years during the pandemic\n\n\n\n\n\n\n\n\nWeil et al. 2024. medRxiv.\n\n\n\n\n\n\n\nUnderdetected dispersal and extensive local transmission drove the 2022 mpox epidemic\n\n\n\n\n\n\n\n\nParedes et al. 2024. Cell.\n\n\n\n\n\n\n\nFitness models provide accurate short-term forecasts of SARS-CoV-2 variant frequency\n\n\n\n\n\n\n\n\nFiggins, Abousamra, and Bedford. 2023. medRxiv.\n\n\n\n\n\n\n\nPositive selection underlies repeated knockout of ORF8 in SARS-CoV-2 evolution\n\n\n\n\n\n\n\n\nWagner et al. 2023. medRxiv.\n\n\n\n\n\n\n\nGenomic surveillance of SARS-CoV-2 Omicron variants on a university campus\n\n\n\n\n\n\n\n\nWeil et al. 2022. Nature communications.\n\n\n\n\n\n\n\nSARS-CoV-2 variant dynamics across US states show consistent differences in effective reproduction numbers\n\n\n\n\n\n\n\n\nFiggins M, Bedford T. 2021. medRxiv\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/ncov-forecasting-fit/ncov-forecasting-fit.html",
    "href": "publications/ncov-forecasting-fit/ncov-forecasting-fit.html",
    "title": "Fitness models provide accurate short-term forecasts of SARS-CoV-2 variant frequency",
    "section": "",
    "text": "*: Co-First Author"
  },
  {
    "objectID": "publications/ncov-forecasting-fit/ncov-forecasting-fit.html#abstract",
    "href": "publications/ncov-forecasting-fit/ncov-forecasting-fit.html#abstract",
    "title": "Fitness models provide accurate short-term forecasts of SARS-CoV-2 variant frequency",
    "section": "Abstract",
    "text": "Abstract\nGenomic surveillance of pathogen evolution is essential for public health response, treatment strategies, and vaccine development. In the context of SARS-COV-2, multiple models have been developed including Multinomial Logistic Regression (MLR) describing variant frequency growth as well as Fixed Growth Advantage (FGA), Growth Advantage Random Walk (GARW) and Piantham parameterizations describing variant Rt. These models provide estimates of variant fitness and can be used to forecast changes in variant frequency. We introduce a framework for evaluating real-time forecasts of variant frequencies, and apply this framework to the evolution of SARS-CoV-2 during 2022 in which multiple new viral variants emerged and rapidly spread through the population. We compare models across representative countries with different intensities of genomic surveillance. Retrospective assessment of model accuracy highlights that most models of variant frequency perform well and are able to produce reasonable forecasts. We find that the simple MLR model provides ~0.6% median absolute error and ~6% mean absolute error when forecasting 30 days out for countries with robust genomic surveillance. We investigate impacts of sequence quantity and quality across countries on forecast accuracy and conduct systematic downsampling to identify that 1000 sequences per week is fully sufficient for accurate short-term forecasts. We conclude that fitness models represent a useful prognostic tool for short-term evolutionary forecasting."
  },
  {
    "objectID": "publications/hct-sars2/hct-sars2.html",
    "href": "publications/hct-sars2/hct-sars2.html",
    "title": "SARS-CoV-2 diversity and transmission on a university campus across two academic years during the pandemic",
    "section": "",
    "text": "Institutions of higher education (IHEs) have been a focus of SARS-CoV-2 transmission studies but there is limited information on how viral diversity and transmission at IHEs changed as the pandemic progressed. Here we analyze 3606 viral genomes from unique COVID-19 episodes collected at a public university in Seattle, Washington (WA) from September 2020 to September 2022. Across the study period, we found evidence of frequent viral transmission among university affiliates with 60% (n=2153) of viral genomes from campus specimens genetically identical to at least one other campus specimen. Moreover, viruses from students were observed in transmission clusters at a higher frequency than in the overall dataset while viruses from symptomatic infections were observed in transmission clusters at a lower frequency. Though only a small percentage of community viruses were identified as possible descendants of viruses isolated in university study specimens, phylodynamic modelling suggested a high rate of transmission events from campus into the local community, particularly during the 2021-2022 academic year. We conclude that viral transmission was common within the university population throughout the study period but that not all university affiliates were equally likely to be involved. In addition, the transmission rate from campus into the surrounding community may have increased during the second year of the study, possibly due to return to in-person instruction."
  },
  {
    "objectID": "publications/hct-sars2/hct-sars2.html#abstract",
    "href": "publications/hct-sars2/hct-sars2.html#abstract",
    "title": "SARS-CoV-2 diversity and transmission on a university campus across two academic years during the pandemic",
    "section": "",
    "text": "Institutions of higher education (IHEs) have been a focus of SARS-CoV-2 transmission studies but there is limited information on how viral diversity and transmission at IHEs changed as the pandemic progressed. Here we analyze 3606 viral genomes from unique COVID-19 episodes collected at a public university in Seattle, Washington (WA) from September 2020 to September 2022. Across the study period, we found evidence of frequent viral transmission among university affiliates with 60% (n=2153) of viral genomes from campus specimens genetically identical to at least one other campus specimen. Moreover, viruses from students were observed in transmission clusters at a higher frequency than in the overall dataset while viruses from symptomatic infections were observed in transmission clusters at a lower frequency. Though only a small percentage of community viruses were identified as possible descendants of viruses isolated in university study specimens, phylodynamic modelling suggested a high rate of transmission events from campus into the local community, particularly during the 2021-2022 academic year. We conclude that viral transmission was common within the university population throughout the study period but that not all university affiliates were equally likely to be involved. In addition, the transmission rate from campus into the surrounding community may have increased during the second year of the study, possibly due to return to in-person instruction."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello and welcome!",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     Github\n  \n  \n    \n     Resume\n  \n  \n    \n     Google Scholar\n  \n\n  \n  \n\n\n\n\nI’m Marlin Figgins, a soon-to-be PhD graduate from the University of Washington’s Department of Applied Mathematics and the Bedford lab at the Fred Hutchinson Cancer Center. My expertise lies at the intersection of statistical modeling, machine learning, and applied mathematics, with a focus on solving complex problems in evolutionary biology and public health. I am eager to apply my expertise to create scalable solutions that drive measured, effective, and fair action!\n\n\n\nDuring my time at the University of Washington and Fred Hutch, I’ve come to believe scientists should seek to transcend traditional research boundaries. This has led to me spearheading interdisciplinary projects, collaborating closely with software engineers to translate complex models into scalable and user-centric tools such as evofr and forecasts-ncov. These projects not only advance our ability to understand, forecast, and monitor viral evolution, but also show how cross-functional collaboration can lead to scientific and practical advancements that can meaningfully improve decision-making.\n\n\n\nAs I transition from academia to the industry, I’m looking for opportunities where I can contribute my expertise in machine learning, statistical analysis, and software engineering to tackle complex problems in any domain. Whether as a research scientist in (bio)tech, a machine learning engineer, or a data scientist, my goal remains the same: to deliver impactful results through innovation, mentorship, and a dedication to technical excellence. I believe that the fusion of quantitative science and domain knowledge is key to solutions that are not only effective but also transformative.\nFor a deeper dive into my professional journey and projects, feel free to explore my Research page or reach out directly via email. Hopefully, we’ll get to work together to build something new and make a difference.\n\n\n\nMy personal email is marlinfiggins [at] gmail [dot] com, but I can also be reached mfiggins [at] uw [dot] edu."
  },
  {
    "objectID": "index.html#most-recent-blog-posts",
    "href": "index.html#most-recent-blog-posts",
    "title": "Hello and welcome!",
    "section": "Most recent blog posts",
    "text": "Most recent blog posts\n\n\n\n\n\n\n\n\n\n\nSimple Bayesian A/B testing\n\n\n\nApr 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Probability I\n\n\n\nAug 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSocial Distancing and Me\n\n\n\nMar 14, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html",
    "href": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html",
    "title": "Social Distancing and Me",
    "section": "",
    "text": "If you’ve been glued to Twitter and/or the rest of the internet like me, you’ve been hearing a lot of talk about SARS-COV-2, social distancing and #FlattenTheCurve. We’ve arrived at the point in this pandemic where our focus has shifted from containing the virus and to slowing its progression. As of writing this, there are 154,219 confirmed cases and nearly 5,700 deaths.\nNaturally, none of us want to see others get infected and we’re all interested in slowing the spread of SARS-COV-2 but in the face of such a scary and global situation, it can be hard to visualize what role you can play in slowing the spread of SARS-COV-2 / COVID-19"
  },
  {
    "objectID": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#where-do-you-fit-into-this",
    "href": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#where-do-you-fit-into-this",
    "title": "Social Distancing and Me",
    "section": "Where do you fit into this?",
    "text": "Where do you fit into this?\nI’m going to try and show you where you may fit into this. No matter how small or large, everyone is a part of a social/contact network of some sort. Parents, friends, roommates, schoolmates, co-workers you name it. If you’re moving around in your daily life, you’re interacting with these people and almost certainly some strangers as well.\n\n\n\n\nYou, your friends, family, and other contacts.\n\n\nLet’s say that the average person has 10 people they consistently interact with in their daily life. That would mean that there’s only 10 direct ways that you can be infected and infect others, but this gets very complicated very quickly. The issue is that our lives aren’t isolated. The people in your life have friends, co-workers, and family of their own in their contact networks. For example, if they have their own 10 people, that means there are 100 people who can infect you indirectly through your contacts. Add another layer and this goes quickly to 1,000 people, 10,000 people, and on-and-on. Theoretically, you and your actions have the ability to affect hundreds of lives as transmission chains begin to grow.\n\n\n\n\nContacts of your contacts"
  },
  {
    "objectID": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#not-everyone-will-be-just-fine.",
    "href": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#not-everyone-will-be-just-fine.",
    "title": "Social Distancing and Me",
    "section": "Not everyone will be “just fine”.",
    "text": "Not everyone will be “just fine”.\nIf you’re watching the news, you’re also seeing several statistics on case fatality rates. Case fatality rates tell us the fraction of confirmed cases that are expected to die. Since only certain infected individuals are tested and confirmed as cases, it’s not quite right to say “if you become infected, you have an X percent chance of dying” because this death rate isn’t distributed evenly between all people who have the infection. As many people have been pointing out, the elderly and immunocompromised are of particularly high risk to SARS-COV-2.\n\n\n\n\nDeaths rates by age group. Source: Chinese CDC via Buzzfeed News\n\n\nEven if you don’t know someone personally who is high-risk, some of your contacts’ contacts may be at higher risk than anyone you interact with daily. They may be immunocompromised due to old age, diseases, disabilities or medications though they may appear “just fine” at a glance and have a much higher chance of having significant complication or dying if they become infected.\n\n\n\n\nFataility rates increase in the immunocompromised."
  },
  {
    "objectID": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#social-distancing-slows-infection-rates.",
    "href": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#social-distancing-slows-infection-rates.",
    "title": "Social Distancing and Me",
    "section": "Social distancing slows infection rates.",
    "text": "Social distancing slows infection rates.\nBy social distancing, you can eliminate possible transmission chains and possibly prevent someone who is at high risk from becoming sick down the line even if you personally would be fine if infected.\n{% include figure image_path=“/assets/images/social-distancing-and-me/sdae-immuno-chain.JPG” height = “200” width = “100” caption=“Possible transmission chain.” %}\n\n\n\nPossible transmission chain.\n\n\nThrough social distancing, not only do you reduce your number of contacts over time and therefore the chance of infecting someone or becoming infected but you also slow the disease’s total spread through your network.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Transmission: Unchecked v.s. Social Distancing"
  },
  {
    "objectID": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#slowing-infection-helps-provide-healthcare-and-treatment-to-those-in-need.",
    "href": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#slowing-infection-helps-provide-healthcare-and-treatment-to-those-in-need.",
    "title": "Social Distancing and Me",
    "section": "Slowing infection helps provide healthcare and treatment to those in need.",
    "text": "Slowing infection helps provide healthcare and treatment to those in need.\nIf fewer people are infected each day, then fewer people need to be hospitalized. This allows healthcare workers to give proper time, attention, and care to those in need as they’re able to replenish their supplies and prevent people from going untreated as new cases come in. This way our local healthcare systems do not become overwhelmed as cases quickly peak. A drawn out, less severe outbreak is preferable in order for people to get access to the care they need if they’re infected. In fact, as we continue to increase the availability of treatment and slow the infection we will likely be able to drive down the mortality rate associated with COVID-19.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Health care demand: Unchecked v.s. Social Distancing"
  },
  {
    "objectID": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#take-a-step-back-flatten-the-curve.",
    "href": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#take-a-step-back-flatten-the-curve.",
    "title": "Social Distancing and Me",
    "section": "Take a step back, flatten the curve.",
    "text": "Take a step back, flatten the curve.\nFlattening the curve (#FlattenTheCurve) is only possible through collective action. Our ability to slow infection and meet the healthcare demands of those in need depends on our willingness to take a step back and focus on keeping each other and especially the most vulnerable of us safe.\n\n\nAll pictures not credited were drawn by me! Thanks for reading.\nThis post was based on a twitter thread of mine embedded below:\n\n\nIf you’ve been glued to Twitter like me, you’ve been hearing a lot of talk about social distancing and #FlattenTheCurve. Naturally, we’re all interested in slowing the spread of SARS-COV-2 but sometimes its hard to visualize how exactly one person can contribute to this. 1/9\n\n— Marlin Figgins (@marlinfiggins) March 14, 2020"
  },
  {
    "objectID": "blog/chaos/chaos-what-is-it-where-to-find-it.html",
    "href": "blog/chaos/chaos-what-is-it-where-to-find-it.html",
    "title": "Chaos: What it is and where to find it",
    "section": "",
    "text": "If we’re interested in taking a first step towards chaos, the usual starting example is the logistic map. A thousand blog posts have been written about this map and chaos in general. Instead of throwing the equation in your face, I’ll try and show this chaos begins to appear in a very simple system. First, let’s start with the simplest possible model for population size. \\(\\newcommand{\\abs}[1]{ \\left| #1 \\right| }\\)"
  },
  {
    "objectID": "blog/chaos/chaos-what-is-it-where-to-find-it.html#the-exponential-map",
    "href": "blog/chaos/chaos-what-is-it-where-to-find-it.html#the-exponential-map",
    "title": "Chaos: What it is and where to find it",
    "section": "The Exponential Map",
    "text": "The Exponential Map\nWhat if every person had on average 1.2 kids every generation? Starting with \\(x_0\\) people, the new population would be \\(1.2 x_0\\) the next generation. After two generations, it’d be \\((1.2)^2 x_0\\) and it’d continue like this for the following generations. We have some growth every generation and it multiplies our current population over and over again. If we want to be more general about this, we can say that the population has a growth parameter \\(\\lambda\\). Then we’d write our population growth each generation as a function\n\\[\n\\begin{equation}\nf(x) = \\lambda x.\n\\end{equation}\n\\]\nIn general, this function is called the exponential map and tells us how big the next generation’s population will be given the population grows by a factor of \\(\\lambda\\). If we keep applying it over and over, we can get the population size several generations in the future. Mathematically, we’d write this as a dynamical system. If we have a starting value or /initial condition \\(x_0\\). Therefore, the population after 1 generation would be \\(f^1(x_0) = \\lambda x_0\\), after two it would be \\(f^2(x_0) = \\lambda^2x_0\\). For simplicity’s sake, we’d say that after \\(n\\) generations the population is\n\\[\n\\begin{equation}\nf^n(x_0)=\\lambda^nx_0.\n\\end{equation}\n\\]\nWe can describe the dynamics of the exponential map by looking at all the population values for each generation\n\\[\\begin{equation}\nx_0, f^1(x_0), f^2(x_0), f^3(x_0), \\dotsc\n\\end{equation}\n\\]\nFrom this, we can begin to see that the dynamics will depend on the value of \\(\\lambda\\) we pick. When dealing with the exponential map, we can have two kinds of outcomes. The population will either die out (\\(0&lt;\\lambda &lt; 1\\)) eventually or it will skyrocket and go to infinity (\\(\\lambda &gt;1\\)). If \\(\\lambda &lt; 1\\), then \\(\\lambda \\cdot \\lambda &lt; \\lambda \\cdot 1\\), so the population shrinks and the population will die out over time. On the other hand, if \\(\\lambda &gt; 1\\), then \\(\\lambda \\cdot \\lambda &gt; \\lambda \\cdot 1\\) so the population grows and continues to grow every generation.\nIn the case we were discussing before, \\(\\lambda\\) would be 1.2 kids per person. Each generation, the population would steadily increase and after 20 generations, we’d have over 30,00 people. After 100, we’d have nearly 80 billion!\n\n\n\nGeneration:\n1\n2\n3\n4\n5\n6\n\n\n\n\nPopulation:\n1000\n1200\n1440\n1720\n2070\n2490\n\n\n\n\n\n\n\nPlotting the exponential map.\n\n\nThis isn’t very realistic because it assumes the population can get infinitely large. When it comes to human population, this probably isn’t realistic. Population growth has to begin to slow down at some point. In this case, there must be some limiting factor to our population growth.\nInstead of doing this by hand each time, we could also use a compute program to simulate these dynamics. We can use the following code to simulate these dynamics. Here’s an example program I wrote below.\ndef exponential(x_0 = 100, lambd = 1, max_n = 10):\n   x = []\n   x = np.append(x, x_0)\n\n   for n in range(max_n - 1):\n       x = np.append(x, lambd*x[n])\n\n   return x"
  },
  {
    "objectID": "blog/chaos/chaos-what-is-it-where-to-find-it.html#logistic-map",
    "href": "blog/chaos/chaos-what-is-it-where-to-find-it.html#logistic-map",
    "title": "Chaos: What it is and where to find it",
    "section": "Logistic Map",
    "text": "Logistic Map\nWhat happens to our model if we assume that resources are somehow limited? As more people appear, it’ll become harder for everyone to meet their individual needs to survive . In order to model this, we need the population change to depend not only on a growth rate, but also how close we are to some theoretical maximum population size. If we say \\(x\\) is the current fraction of this maximum population size, we might change our growth rate from \\(\\lambda\\) to \\(\\lambda (1-x)\\) to allow for the growth rate to decrease as the population increases. Therefore, our model would become\n\\[\n\\begin{equation}\nf(x) = \\lambda(1-x) x.\n\\end{equation}\n\\]\nThis is called the logistic map. Strictly speaking, it is a family of maps depending on different values of \\(\\lambda\\). In order to see what happens to the population each generation, we want to analyze the dynamics like in the last example. In the last example, we were able to write a formula for the dynamics and saw the population would either blow up to infinity or die out eventually. This time around we can’t easily write an explicit equation for the dynamics. One usual tool in cases like these where we can’t easily write down a solution is simulation. We can rely on code to help us visualize the population dynamics over time. Here’s the program I use to the simulate the logistic map in the figures below.\n## Logistic Map\ndef logistic_map(x_0 = 0.4, lambd = 1.2, max_n = 500):\n    x = []\n    x = np.append(x, x_0)\n\n    for n in range(max_n - 1):\n        x_new = lambd*x[n]*(1-x[n])\n        x = np.append(x, x_new)\n    return(x)\nDespite having the tools to simulate this, let’s try to understand this at an intuitive level first since all good code needs a sanity check. We made this model to account for limits on a population and picked a decreasing growth rate, so you might expect that the dynamics will reflect this and keep the population in check. Even though this works on an intuitive level, we should still try and observe it mathematically. Following this intuition, the first thing we should check is whether our population will stabilize as we expect. Much like in the case of the exponential map, this all depends on the value of the growth parameter \\(\\lambda\\).\nIt’s actually not that hard to prove that there is some point of stability where the population will stay the same from year to year. In mathematical terms, this is called a fixed point. From our point of view, we say the population \\(x_\\star\\) is fixed if \\(f(x_\\star) = x_\\star\\).\nProposition: Every continuous function \\(f\\) takes points from \\([0,1]\\) to \\([0,1]\\) has a fixed point.\nProof. Let’s define a new function \\(D(x) = f(x) - x\\). Since \\(f(0)\\) must be in \\([0,1]\\), it must be greater than or equal to 0. Therefore, we know \\[D(0) = f(0) - 0 \\geq 0.\\] Similarly, \\(f(1)\\) is in \\([0,1]\\), so we know that \\(D(1)&lt;0\\). Since \\(f\\) is continuous, the intermediate value theorem guarantees us that there is a fixed point \\(x_\\star\\).\nIn order for this to hold for the logistic map, we need to make sure that \\(f(x)\\) stays between 0 and 1 which means that \\(\\lambda\\) must be between 0 and 4. Under these assumptions, we can solve for our fixed point(s) directly. One obvious fixed point is \\(x_\\star = 0\\), but let’s see if there are any others. If \\(x_\\star\\) is a non-zero fixed point of \\(f(x)\\), then we need \\(\\lambda(1-x)x = x\\). Dividing by \\(x\\), we get \\(\\lambda(1-x) = 1\\). Multiplying this out, we get \\(\\lambda - 1 = \\lambda x\\) which gives shows that our equilibrium value must be\n\\[\n\\begin{equation}\nx_\\star = 1 - \\frac{1}{\\lambda}.\n\\end{equation}\n\\]\nNotice that in order for this fixed point to be greater than 0, we need \\(\\lambda\\) to be greater than 1. This makes sense since otherwise we can expect the population to die out like in the exponential case.\nWe know it’s possible for our population to stabilize, but when will it? We can use the derivative of this function to help us discern whether this point is attracting i.e. whether initial conditions near it will approach this fixed point.\nProposition: If \\(x_\\star\\) is a fixed point and \\(\\abs{ f'(x_\\star)} &lt; 1\\), then it is an attractive fixed point.\nProof. In calculus, we learn that the derivative is given by\n\\[\n\\begin{equation}\nf'(x_\\star) = \\lim\\limits_{n\\to \\infty} \\frac{f(x) -f(x_\\star)}{x-x_\\star}.\n\\end{equation}\n\\]\nThis tells us that in some neighborhood of \\(x_\\star\\), \\(f\\) is a contraction. In technical terms, there exists some \\(\\delta &gt; 0\\) such that for \\((x_\\star - \\delta, x_\\star + \\delta)\\),\n\\[\n\\begin{equation}\n\\abs{f(x) - f(x_\\star)} &lt; \\abs{ x-x_{\\star}}.\n\\end{equation}\n\\]\nThis means that if we pick a initial condition \\(x_0\\) that is close enough to \\(x_\\star\\), then \\(x_0\\) will get closer to \\(x_star\\) after each iteration or generation in our case.\nThis makes our problem much easier. If we want to find which \\(\\lambda\\) values have attracting fixed points, then we just check to see when \\(\\abs{ f'(x_\\star)} &lt; 1\\). Taking the derivative of \\(f\\), we get \\(f'(x) = \\lambda(1-2x)\\). Plugging in our \\(x_\\star\\) value, we see that\n\\[\n\\begin{equation}\nf'\\left(1- \\frac{1}{\\lambda}\\right) = \\lambda\\left( 1 - 2\\left(1- \\frac{1}{\\lambda}\\right)\\right) = \\lambda\\left(\\frac{2}{\\lambda} - 1\\right) = 2 - \\lambda.\n\\end{equation}\n\\]\nThis tells us that the fixed point is attractive for \\(\\lambda\\) between 1 and 2 or between 2 and 3. Now that we’ve confirmed the existence of a fixed point and know a bit about the dynamics of this system, let’s do some simulations to see what happens to our populations over several generations.\n\n\n\n\nPopulation dynamics for different growth parameters\n\n\nAs expected from the math we did above, \\(\\lambda\\) values in \\((1,2)\\) and \\((2,3)\\) approach the fixed point \\(1 - \\frac{1}{\\lambda}\\). Relating this back to our original example, this tells us that having limited resources will cause our population to peak and stabilize. We can see this in the plot above, where \\(\\lambda_2 = 1.2\\). Looking into the rest of the cases, we can also notice that for \\(\\lambda &lt; 1\\), the population dies out which makes sense biologically. A population that can’t successfully replenish itself is going to go extinct. What is interesting here are the values of \\(\\lambda\\) that are greater than 3. The population appears to be oscillating periodically for \\(\\lambda_4 = 3.2\\), \\(\\lambda_5 = 3.48\\), and possibly for \\(\\lambda = 3.56\\) which corresponds to the population constantly overshooting its threshold and falling back down. When we get to even higher \\(\\lambda\\) values, these population dynamics seems to fluctuate like crazy, significantly departing from the cases we looked at before."
  },
  {
    "objectID": "blog/chaos/chaos-what-is-it-where-to-find-it.html#bifurcations",
    "href": "blog/chaos/chaos-what-is-it-where-to-find-it.html#bifurcations",
    "title": "Chaos: What it is and where to find it",
    "section": "Bifurcations",
    "text": "Bifurcations\nLet’s try and visualize this behavior as \\(\\lambda\\) changes with a bifurcation diagram. A bifurcation diagram helps give us an idea about where our system will eventually end up. We take several trajectories for different \\(\\lambda\\) values and plot these \\(\\lambda\\) values against where the corresponding population ends up going in the long term. This tells us how the asymptotic behavior of our system changes alongside our growth parameter \\(\\lambda\\).\n## Values of lambda to sweep over\nlambda_range = np.linspace(2, 4, 1000)\ntail = 200\nlambd_seq = []\neq_seq = []\n\n## Run the model for 1000 generations and take at the last 200 values.\nfor lambd in lambda_range:\n    out = logistic_map(lambd = lambd, max_n = 600)\n    lambd_seq = np.append(lambd_seq, [lambd]*tail)\n    eq_seq = np.append(eq_seq, out[-tail:])\n\n\n\n\nBifurcation diagram for the logistic map\n\n\nNotice where the lines for each of our \\(\\lambda\\) values intersect. This tells us about the end behavior of the dynamics for that growth parameter value. For example, notice that \\(\\lambda_4\\) intersects the curve twice and the population dynamics seem to oscillate with a period of 2. Similarly, \\(\\lambda_5\\) intersects 4 times and we see it has period 4. We can use this idea to confirm our earlier suspicion that the dynamics for \\(\\lambda_6 = 3.56\\) are periodic. Looking at the bifurcation diagram, the line for \\(\\lambda_6\\) appears to intersect 8 times meaning the population dynamics should have period 8. This leaves us to analyze the higher values of \\(\\lambda\\). As you can see, the plot gets messy as \\(\\lambda\\) gets higher. Based on what we saw with the previous cases, there are now many, many points that your population dynamics are going to bounce between and everything becomes extremely irregular. This is chaos."
  },
  {
    "objectID": "blog/chaos/chaos-what-is-it-where-to-find-it.html#sensitivity-to-initial-conditions",
    "href": "blog/chaos/chaos-what-is-it-where-to-find-it.html#sensitivity-to-initial-conditions",
    "title": "Chaos: What it is and where to find it",
    "section": "Sensitivity to Initial Conditions",
    "text": "Sensitivity to Initial Conditions\nUp until now, our choice in initial conditions didn’t have much of an effect on where our system ended up. We can see this is the case when we’re working with \\(\\lambda\\) values less than 3. Once we begin to move our \\(\\lambda\\) values above 3, it becomes increasingly important to be precise with our choice of initial conditions \\(x_0\\). As you can see below, different but very close initial conditions \\(x_0\\) can lead to very different dynamics. This property is called sensitivity to initial conditions and is one of the hallmarks of chaos.\n\n\n\n\nVarious initial conditions for \\(\\lambda =3.95\\).\n\n\nHere, we see that the populations do diverge significantly even if they start close to one another, but this doesn’t give an idea of how quickly this occurs. Let’s take two initial conditions \\(x_0\\) and \\(x_0'\\) see how their populations change across several generations. For simplicity, let’s say that \\(x_{0} = x_{0}' + \\Delta x\\). At each generation, we’ll check to see how different the population levels are and plot this difference. Here’s the program I used to do this.\nmax_n = 40\nx_0 = 0.7\npower = -10\nDelta_x = 10**(power)\nIC = [x_0, x_0 + Delta_x]\ntime =  np.linspace(1, max_n, max_n)\n\n\nout1 = logistic_map(lambd = 3.95, max_n = max_n, x_0 = IC[0])\nout2 = logistic_map(lambd = 3.95, max_n = max_n, x_0 = IC[1])\ndist = abs(out1 - out2)\n\n\n\n\nPopulation differences for similar initial conditions.\n\n\nNotice that the difference between the populations appears to grow exponentially in time for a bit. Let’s try to get a measure of how exponential this. If we try to write the distance as \\(\\text{Dist}(t) \\approx Ce^{\\chi t}\\) for some constants \\(C\\) and \\(\\chi\\). Taking the logarithm of this expression, we can see \\[\n\\begin{equation}\n\\log(\\text{Dist}) \\approx \\chi t + \\log(C).\n\\end{equation}\n\\]\nWith it written this way, we see this is equivalent to saying \\(\\log(\\text{Dist})\\) is approximately linear in time, so we can do a linear regression to estimate the exponential rate \\(\\chi\\).\nplt.rcParams['figure.figsize'] = [12, 9]\nchi_estimate, logC, r_value, p_value, std_err = stats.linregress(time,np.log(dist))\n\nlin_reg=chi_estimate*time+logC\n\n\n\n\nLog population differences with linear regression.\n\n\nThis leaves us with \\(\\chi \\approx 0.566\\). This number \\(\\chi\\) is called a Lyapunov exponent. In more general contexts, it is used to describe how quickly extremely close trajectories diverge from one another.\nAs of now, we’ve been able to analyze the behavior of the logistic map, which can be used to describe how populations grow in conditions with limited resources. We saw that the growth parameter \\(\\lambda\\) of a population determines how it will grow over time and proved that for some \\(\\lambda\\) values the population will stabilize at a fixed value. We also showed that some growth parameters cause the population to overshoot and undershoot consistently leading to oscillation and that for even higher values we end up with chaos. We were able to visualize how this occurs. We explored what it meant to be chaotic by describing sensitivity to initial condition and getting a measure of how sensitive a system is. This was an exercise in using math to take a stab a question we might have about the real world. First, we made a crude model of how we think populations should change every generation, then we went down the rabbit hole of analyzing it. On the way, we got to see some very pretty math and make some pretty cool plots.\nThat being said, this is not the end-all-be-all of chaos. Chaos is everywhere: in population dynamics with the logistic map, in physics with things like the double pendulum, weather as in the Lorenz equations, and often in infectious disease models. They’re so many chaotic systems out in the world, and I’m itching to write some posts about others that I have or will come across. This also won’t be the last we see of the logistic map. I’m hoping to write a post about the Mandelbrot set, where it comes from, a bit on its connection to the logistic map. For now, this has been an introduction to chaos.\nFrom what we’ve seen in this introduction, chaos is not simply something changing without any rhyme or reason. It must be related to an underlying rule or process describing how things change throughout time or space for a system. Chaos is the idea that uncertainty can grow and grow exponentially quickly. Chaos is realizing that knowing where you are now doesn’t always tell you exactly where you have been in the past. It reminds us that starting from similar conditions or backgrounds doesn’t dictate that you’ll end up in the same place as one another. It tells us that all of our possible paths may at first diverge yet still possibly intersect, intertwine, and become parallel in the future. As you continue along in a chaotic system, it feels as though the future is completely unknown with no clarity to be had. This is when we’re meant to realize that the path ahead is still determined by some underlying rule, even if we don’t understand it yet.\nUntil next time,\nMarlin F."
  },
  {
    "objectID": "blog/ab_testing_intro/ab_testing_intro.html",
    "href": "blog/ab_testing_intro/ab_testing_intro.html",
    "title": "Simple Bayesian A/B testing",
    "section": "",
    "text": "Choosing between alternatives is a fairly common task in daily life. Oftentimes, we want to develop some notion of how and why one alternative may be better than another. In the absence of data, we can design and run tests on these alternatives to better understand them.\nIn the arena of business decision-making, we often have some metric of interest. This may be the click-through rate on an ad, the color of a button, article headlines, or comparing coupon-types. These examples all fall into the larger bucket of .\nToday, we’ll focus on a pretty fun synthetic example to illustrate how we might begin to A/B test.\nLet’s suppose that we’re a rollerskating retailer who is testing out two ad variants in anticipation for the inevitable global resurgence of roller skating.\nWe have two ad variants prepared. The first ad variant (Winter Wonderland) features Mariah Carey in a parka rollerskating in a snow globe. The second ad variant (Summer Renaissance) features Beyonce rollerskating beachside on a sunny day."
  },
  {
    "objectID": "blog/ab_testing_intro/ab_testing_intro.html#ab-testing",
    "href": "blog/ab_testing_intro/ab_testing_intro.html#ab-testing",
    "title": "Simple Bayesian A/B testing",
    "section": "",
    "text": "Choosing between alternatives is a fairly common task in daily life. Oftentimes, we want to develop some notion of how and why one alternative may be better than another. In the absence of data, we can design and run tests on these alternatives to better understand them.\nIn the arena of business decision-making, we often have some metric of interest. This may be the click-through rate on an ad, the color of a button, article headlines, or comparing coupon-types. These examples all fall into the larger bucket of .\nToday, we’ll focus on a pretty fun synthetic example to illustrate how we might begin to A/B test.\nLet’s suppose that we’re a rollerskating retailer who is testing out two ad variants in anticipation for the inevitable global resurgence of roller skating.\nWe have two ad variants prepared. The first ad variant (Winter Wonderland) features Mariah Carey in a parka rollerskating in a snow globe. The second ad variant (Summer Renaissance) features Beyonce rollerskating beachside on a sunny day."
  },
  {
    "objectID": "blog/ab_testing_intro/ab_testing_intro.html#collecting-data",
    "href": "blog/ab_testing_intro/ab_testing_intro.html#collecting-data",
    "title": "Simple Bayesian A/B testing",
    "section": "Collecting data",
    "text": "Collecting data\nWe want to collect some data here to see how each of the ads perform, so we run a test where we try out each ad variant on customers over some period of time.\nBefore customers are shown an ad, we randomize which ad variant they are shown and keep track of both how many times the ad is shown (impressions) and how many times it is clicked (clicks). This is to ensure that we’re getting a good sample of each ad across all potential customers which may differ in their underlying click-through rates.\nAfter this process, we may get data that looks something like this.\n\n\n\nclicks\nimpressions\nvariant\n\n\n\n\n2\n120\nWW\n\n\n3\n143\nSR\n\n\n\nWe now need to analyze this data to get some idea of the ."
  },
  {
    "objectID": "blog/ab_testing_intro/ab_testing_intro.html#estimating-click-through-rate",
    "href": "blog/ab_testing_intro/ab_testing_intro.html#estimating-click-through-rate",
    "title": "Simple Bayesian A/B testing",
    "section": "Estimating click-through rate",
    "text": "Estimating click-through rate\nThe variable of interest here is the click-through rate. We want to pick the ad variant which maximizes the click-through rate.\nWe can estimate the click-through rate by treating the number of clicks as a binomial distribution. We can think of this as the number of clicks assuming that each time the ad is shown it is clicked with some probability, so that we have\n\\[\n\\text{clicks} \\sim \\text{Binomial}(\\text{total}, p),\n\\]\nwhere \\(p\\) is our click-through rate i.e. the probability that a user clicks on the ad they’re shown.\nIf we also have some prior expectation of the CTR, we can combine this with the likelihood to create a Bayesian model.\nSince the CTR is between 0 and 1, we can choose a beta distribution as the prior.\n\\[\n\\text{CTR} \\sim \\text{Beta}(\\alpha, \\beta).\n\\]\nThis particular combination of prior and likelihood allows us to compute the posterior distribution of the CTR analytically:\n\\[\n\\text{CTR} \\sim \\text{Beta}(\\alpha + \\text{counts}, \\beta + \\text{totals} - \\text{counts}).\n\\]\nThis is an example of conjugate priors which you can read more about here. Typically, \\(\\alpha = \\beta = 1\\) is used for the prior here and acts a weak uniform prior on the click-through rate.\n\n\n\nPrior and posterior estimates of click-through rate from above model.\n\n\nHowever, the choice of prior does matter here. The choice of prior can bias or regularize your results towards particular values. This is useful when you have a strong idea or prior expectation for the values of your parameters. For example, if prior experiments tell you that your click-through rate is between 0.01 and 0.23, it might be useful to encode this as prior information.\n\n\n\nChanging prior affects the computed posterior distribution."
  },
  {
    "objectID": "blog/ab_testing_intro/ab_testing_intro.html#which-is-better",
    "href": "blog/ab_testing_intro/ab_testing_intro.html#which-is-better",
    "title": "Simple Bayesian A/B testing",
    "section": "Which is better?",
    "text": "Which is better?\nWe’ve shown that we can use this model to estimate the click-through rate, but this doesn’t answer the question we actually care about i.e. “Which one should we choose?”\nWe can begin by sampling the difference between the click-through rates of the alternatives. This is called the lift.\n\\[\n\\text{lift}_{\\text{SR}, \\text{WW}} = \\text{CTR}_{\\text{SR}} - \\text{CTR}_{\\text{WW}}.\n\\]\nIn our case, since we have random samples of the CTR from each options, we can compute a distribution of lift by computing lift from individual samples from our posterior distributions.\n\n\n\nComputed lift from posterior samples of CTR.\n\n\nThis is step one: we now have reduced the posterior into a metric that we care about. The next step is figuring out how to make a decision based on this metric.\n\nDecision theory and expected loss\nWe can think of this in terms of minimizing the loss in outcome from choosing a particular option. To apply this to our example, we might ask, “What would we lose if we choose Winter Wonderland as our main ad campaign?”. Well, this will be the lift of the best option over winter wonderland.\n\\[\n\\text{loss}_{\\text{WW}} = \\text{lift}_{\\text{BEST}, \\text{WW}} = -\\text{lift}_{\\text{WW}, \\text{BEST}}.\n\\]\nSince we’re dealing with random samples of loss, we often work with the expected (or mean) loss:\n\\[\n\\newcommand{ \\Expect }{ \\mathbb{E} }\n\\begin{align}\n\\Expect[\\text{loss}_\\text{WW}] &= \\frac{1}{S} \\sum_{s=1}^{S} \\text{loss}^{s}_{\\text{WW}} \\\\\n                               &= \\frac{1}{S} \\sum_{s=1}^{S}  -\\text{lift}^{s}_{\\text{WW}, \\text{BEST}}\\\\\n                               &= \\frac{1}{S} \\sum_{s=1}^{S} \\max_{A} \\left\\{-\\text{lift}^{s}_{\\text{WW}, A} \\right\\}.\n\\end{align}\n\\]\nwhere we’ve computed the loss for each posterior sample \\(s\\).\n\n\n\nLoss computed among alternatives.\n\n\nNow that we understand how much we can expect to lose by selecting an option, we have to decide when it is it worth it to switch. For example, if we only stand to make $2 from the change, it probably isn’t worth because the alternatives may be practically equivalent. This is where the idea of the threshold of caring comes in. The threshold of caring determines the level of expected loss below which you consider alternatives practically indistinguishable. This also has the effect of being a check on the acceptability of an outcome since alternatives below this threshold are the best alternatives given that threshold. In practice, the threshold of caring can be determined by considering the practical consequences of alternatives e.g. considering lift and loss in terms of revenue.\n\n\n\nExpected loss and threshold of not caring. Outcomes below the threshold are acceptable outcomes.\n\n\nSince the expected loss of Summer Renaissance is below the threshold of caring, we can say that Summer Renaissance is an acceptable outcome and choose that as our final outcome. If there were multiple acceptable outcomes, we could simply take the one with the lowest expected loss."
  },
  {
    "objectID": "blog/ab_testing_intro/ab_testing_intro.html#implementation-in-python",
    "href": "blog/ab_testing_intro/ab_testing_intro.html#implementation-in-python",
    "title": "Simple Bayesian A/B testing",
    "section": "Implementation in Python",
    "text": "Implementation in Python\nWe can wrap all these ideas into a simple class that runs this rate experiment:\n\n\nShow the code\nfrom scipy.stats import beta, beta_gen\nimport numpy as np\nimport pandas as pd\n\nclass SimpleRateExperiment:\n    \"\"\"\n    A class to conduct Bayesian A/B testing for categorical experiment outcomes based on rate metrics such as click-through rates.\n    \n    This class calculates the posterior distributions for conversion rates using a Beta distribution, samples from these distributions to estimate performance differences (lifts), and then makes decisions based on expected losses and specified thresholds of caring.\n    \"\"\"\n\n    def __init__(self, counts: str, totals: str, group: str, a_prior: int = 1, b_prior: int = 1, size: int = 1000):\n        \"\"\"\n        Constructs all the necessary attributes for the SimpleRateExperiment object.\n        \n        Parameters:\n            counts (str): The column name for the count of successes (e.g. clicks).\n            totals (str): The column name for the total attempts (e.g. impressions).\n            group (str): The column name to group data by, typically representing different experiment variants.\n            a_prior (int): The alpha parameter of the prior Beta distribution (default 1).\n            b_prior (int): The beta parameter of the prior Beta distribution (default 1).\n            size (int): The number of samples to draw from each posterior (default 1000).\n        \"\"\"\n        self.counts, self.totals = counts, totals\n        self.group = group\n        self.a_prior = a_prior\n        self.b_prior = b_prior\n        self.size = size\n        self.posterior: dict[str, beta_gen] = {}\n        self.samples: dict[str, np.ndarray] = {}\n        self.expected_losses: dict[str, float] = {}\n\n    def _compute_posterior(self, data: pd.DataFrame):\n        \"\"\"\n        Computes the posterior distribution of each group using the Beta distribution based on the provided data.\n\n        Parameters:\n            data (DataFrame): Pandas DataFrame containing experiment data with columns for counts and totals.\n        \"\"\"\n        for name, group in data.groupby(self.group):\n            self.posterior[name] = beta(self.a_prior + group[self.counts], self.b_prior + group[self.totals] - group[self.counts])\n    \n    def _sample_posterior(self):\n        \"\"\"\n        Samples from the posterior distributions of each group.\n        \"\"\"\n        for name, post in self.posterior.items():\n            self.samples[name] = post.rvs(self.size)\n\n    def _compute_lift(self, current: str, alternative: str) -&gt; np.ndarray:\n        \"\"\"\n        Computes the lift of an alternative group over the current group by calculating the difference in samples from their posterior distributions.\n\n        Parameters:\n            current (str): The current group name.\n            alternative (str): The alternative group name to compare against.\n\n        Returns:\n            np.ndarray: Differences in sample values representing the lift.\n        \"\"\"\n        return self.samples[alternative] - self.samples[current]\n\n    def decide(self, thres_caring: float = 0.01):\n        \"\"\"\n        Decides the best group based on the expected loss being below a certain threshold.\n\n        Parameters:\n            thres_caring (float): The threshold below which an alternative's expected loss is considered acceptable.\n        \"\"\"\n        # Compute expected loss\n        for current, post in self.posterior.items():\n            lifts = np.asarray([self._compute_lift(current, alt) for alt in self.posterior.keys()]) \n            self.expected_losses[current] = -(-lifts).min(axis=0).mean()\n        \n        # Check to see which alternatives if any meet threshold of caring\n        threshold_met = False\n        for alt, expected_loss in self.expected_losses.items():\n            if expected_loss &lt; thres_caring:\n                print(f\"{alt} is acceptable\")\n                threshold_met = True\n        if threshold_met:\n            self.decision = min(self.expected_losses, key=self.expected_losses.get)\n            print(f\"We choose {self.decision}.\")\n        else:\n            print(\"No alternatives met the threshold of caring.\")\n    \n    def run_test(self, data: pd.DataFrame, thres_caring: float = None):\n        \"\"\"\n        Executes the test by computing posterior distributions, sampling them, and possibly making a decision based on a threshold.\n\n        Parameters:\n            data (DataFrame): Pandas DataFrame containing experiment data.\n            thres_caring (float, optional): The threshold of caring to use when making a decision. If None, no decision is made.\n        \"\"\"\n        self._compute_posterior(data)\n        self._sample_posterior()\n\n        if thres_caring is not None:\n            self.decide(thres_caring)\n\n\nWith this class running an experiment becomes as simple as loading our data and using the run_test method:\n\nexperiment = SimpleRateExperiment(counts=\"clicks\", totals=\"impressions\", group=\"variation\")\nexperiment.run_test(data, thres_caring=0.01)\n\n\nConclusion\nThis post is an introduction to some of the ideas behind A/B testing in a Bayesian framework. There are several common pitfalls that are discussed when it comes to A/B testing such as the effects of peeking which we do not discuss here. Today, I’m focusing on applying the ideas of Bayesian decision theory to the simplest type of Bayesian A/B test where we have nice likelihood which allow us to take advantage of conjugate priors. I plan to discuss extensions to this method in the future including real-time allocation of variations with multi-armed bandits. For now, I hope you have enjoyed this post and I look forward to writing more on this topic in the future."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Marlin Figgins",
    "section": "",
    "text": "Who am I?\nMy name is Marlin Figgins and welcome to my blog. I am currently a final year PhD candidate in the University of Washington’s Department of Applied Mathematics as a Boeing, ARCS, and NSF GRFP fellow.\nI currently develop Bayesian methods for inference and forecasting of epidemic and evolutionary dynamics of respiratory viruses in the Bedford lab at the Fred Hutchinson Cancer Research Center. You can learn about this work and more on the Research page.\nFor more on my professional life, you can check out my CV / Resume in PDF format on this page.\nGet in contact? Stay in contact?\nIf you need to contact me for any reason, my personal email is marlinfiggins [at] gmail [dot] com, but I can also be reached mfiggins [at] uw [dot] edu."
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "Blog posts",
    "section": "",
    "text": "Stats\n\n\n\n\nApr 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMathematics\n\n\nStatistics\n\n\n\n\nAug 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEpidemiology\n\n\n\n\nMar 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMath\n\n\n\n\nJul 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMath\n\n\n\n\nMay 2, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/blog.html#my-blog-posts",
    "href": "blog/blog.html#my-blog-posts",
    "title": "Blog posts",
    "section": "",
    "text": "Stats\n\n\n\n\nApr 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMathematics\n\n\nStatistics\n\n\n\n\nAug 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEpidemiology\n\n\n\n\nMar 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMath\n\n\n\n\nJul 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMath\n\n\n\n\nMay 2, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/dive-into-caluclus-of-variations/dive-into-the-calculus-of-variations.html",
    "href": "blog/dive-into-caluclus-of-variations/dive-into-the-calculus-of-variations.html",
    "title": "A Dive into the Calculus of Variations",
    "section": "",
    "text": "For those who don’t follow me on Twitter, I wrote a thread about Dr. J. Ernest Wilkins Jr. a Black mathematician who one of my professors took the time to tell me about one day after class. I wrote about how it made me feel as if I was being recognized for doing mathematics well for one of the first time ever. It reminded me that someone like me could encounter beautiful mathematics, and in particular, it reminded me that people can inspire one another and set an example for those who come after. Because of this, I set a very ambitious goal for myself. I promised to write a set of notes dedicated to the calculus of variations in the memory of Dr. Wilkins Jr.\nI’ve been working on this for a while and frankly struggling. Finding the time to write this set of notes has been difficult, I’ve mostly been working an outline and surveying introductory texts to figure out why the Calculus of Variations is (or isn’t) interesting to study. One thing that’s apparent in anything I saw, at least in anything I could understand, was the natural physical beauty motivating some of the theory as well the complexity of the field and delicate treatment that it demands. This is the main reason why getting a proper start has been particularly challenging for me. I wanted to be confident that I can give the material justice. What I forgot is that there’s a beauty in trying and failing. I forgot that no matter how hard it is at first to start this kind of project you can never know how enjoyable or rewarding it can become until you begin. I’ll try and let my hard work speak for itself in this. As a start to this, I’ll tell you how I became interested in this project and introduce some basic ideas of and provide some motivation for the calculus of variations."
  },
  {
    "objectID": "blog/dive-into-caluclus-of-variations/dive-into-the-calculus-of-variations.html#example",
    "href": "blog/dive-into-caluclus-of-variations/dive-into-the-calculus-of-variations.html#example",
    "title": "A Dive into the Calculus of Variations",
    "section": "Example",
    "text": "Example\nDisclaimer: I am not an economist by any means, but I figured I’d take a shot at something unfamiliar!!\nYou’re ordering shot glasses and are trying to figure out the best number to order to maximize profit. Let’s say you have 150 shot glasses to sell and want to sell them for at most 3 dollars per shot glass. If you know that you will have 50 loyal customers who will buy a shot glass regardless of price and that you will sell 5 more each time you decrease the price by 5 cents, how can you maximize your revenue? Using the fact that revenue is price times the quantity sold, we can write our revenue \\(R\\) as:\n\\[\n\\begin{equation}\nR(n)= (3-0.05n)(50+5n)=150 + 12.5n - 0.25n^2.\n\\end{equation}\n\\]\nWe can take the derivative of this and set it equal to 0 to find the optimal value of \\(n\\) which maximizes our profit.\n\\[\\begin{equation}\n\\frac{dR}{dt}= 12.5-0.5n=0\n\\end{equation}\\]\nIn this case, we see that the optimal value of \\(n\\) is 25. That is, to maximize our revenue we must sell 75 shots glasses total. Because of this, we should set a minimum price of $1.75 to prevent losing out on profit from the price decreasing further!\n\n\n\nGraph of \\(R(n)\\). Notice the peak!\n\n\nThis is the case where we’re only selling one object. What if we wanted to sell shot glasses and teddy bears? Then our revenue and pricing depends on two variables: the number of shot glasses and the number of teddy bears. Using multivariable calculus, we can solve this kind of problem! I will not do this example in full, but it’s something fun to figure out how to solve! Hint: Find the stationary values for shot glasses and teddy bears independently.\n\n\n\nVisualizing the case with shot glasses and teddy bears\n\n\nThese examples all occur in finite dimensional spaces, but as we’ll see, moving to spaces with infinitely many dimensions forces to change our methodology a bit since our current methods depend on finding solutions to a finite number of equations."
  },
  {
    "objectID": "blog/dive-into-caluclus-of-variations/dive-into-the-calculus-of-variations.html#function-spaces-and-functionals",
    "href": "blog/dive-into-caluclus-of-variations/dive-into-the-calculus-of-variations.html#function-spaces-and-functionals",
    "title": "A Dive into the Calculus of Variations",
    "section": "Function Spaces and Functionals",
    "text": "Function Spaces and Functionals\nFor example, let’s say we want to find the path that gives the minimum distance between the points \\((a, \\alpha)\\) and \\((b, \\beta)\\). Intuitively, we know the answer to this is a straight line, but to begin attacking this question, it makes sense to restrict the kinds of paths we can take to only those that are reasonable. In particular, we restrict our attention to the following set of functions:\n\\[\\mathscr{F}=\\{f\\colon [a,b]\\to \\mathbb{R} \\mid f(0)=\\alpha, f(1)=\\beta, \\text{ and $f$ is differentiable}\\}.\\]\nIn order to find the curve which minimizes the distance between two points, it’s then our goal to find the function(s) with the minimum arc length. We can represent the arc length \\(s\\) from \\(a\\) to \\(b\\) as \\[s=\\int_{a}^{b}ds,\\] where \\(ds\\) represents an infinitesimal arc length. Motivated by the Pythagorean theorem, we can draw a triangle with sides \\(dx\\) and \\(dy\\) and hypotenuse \\(ds\\). We write this as \\(ds^2=dx^2+dy^2\\). Using that \\(y\\) is a function of \\(x\\), we can rewrite this as:\n\\[ds=\\sqrt{dx^2+dy^2}=\\sqrt{1+\\left(\\frac{dy}{dx}\\right)^2}dx.\\]\n\nTherefore, given function \\(y(x)\\), we can write its arc length as:\n\\[\n\\begin{equation}\ns[y]=\\int_{a}^{b}\\sqrt{1+\\left(\\frac{dy}{dx}\\right)^2}dx.\n\\end{equation}\\]\nTo approach this problem, we’ve shifted to finding functions that minimize functionals or functions of functions. That still doesn’t answer the question of how to find these optimal functions though.\nLet’s consider a more general case. Suppose we have a space of functions \\(\\mathscr{F}\\) consisting of “reasonable” candidate functions \\(f\\) defined on a space \\(E\\) and a functional\n\\[\\begin{equation}\nI[f]=\\int_E F(x,f, f')dx,\n\\end{equation}\\]\nfor some function \\(F\\) which depends on \\(x\\), the function \\(f\\), and its derivative \\(f'\\). We can try and find the derivative of this functional mimicking what we’ve learned in calculus and saw in our first example, but how can we compute the ‘derivative’ in this case."
  },
  {
    "objectID": "blog/dive-into-caluclus-of-variations/dive-into-the-calculus-of-variations.html#the-euler-lagrange-equation",
    "href": "blog/dive-into-caluclus-of-variations/dive-into-the-calculus-of-variations.html#the-euler-lagrange-equation",
    "title": "A Dive into the Calculus of Variations",
    "section": "The Euler-Lagrange Equation",
    "text": "The Euler-Lagrange Equation\nOne criterion is that any stationary point or extremal function \\(f\\) satisfies the following equation when the functional \\(I[\\cdot]\\) of the form above:\n\\[\\begin{equation}\n\\frac{d}{dx}\\left(\\frac{\\partial F}{\\partial y'} \\right)-\\frac{\\partial F}{\\partial y}=0.\n\\end{equation}\\]\nThis is called the Euler-Lagrange equation. It’s useful for solving this kinds of optimization problems because it helps one identify local extrema i.e. candidates for maxima and minima, but the Euler-Lagrange equation is not sufficient to identify whether functions are maxima or minima or neither1.\nIt is however, extremely useful in the example we started above! Shifting our focus back to the arc length example. Taking partial derivatives, we see that \\[\n\\frac{\\partial F}{\\partial y'}=\\frac{y'}{\\sqrt{1+y'^2}}.\n\\] We can see the second term of the Euler-Lagrange must be zero since \\(F(x,f,f')\\) does not depend on the function \\(y\\) itself in this case. Simplifying, we see that\n\\[\\frac{d}{dx}\\left(\\frac{y'}{\\sqrt{1+y'^2}} \\right)= 0\\]\nfor any extremal function \\(y\\). Therefore, \\(\\frac{y'}{\\sqrt{1+y'^2}}\\) must be constant. Rearranging, we see that \\(y'(x)\\) itself must be some constant \\(m\\). Therefore, after integrating, we find that \\[y(x)=mx+b \\] for constants \\(m\\) and \\(b\\). Plugging in the boundary conditions from our function space \\(\\mathscr{F}\\) gives us that our extremal function \\(y\\) is:\n\\[\\begin{equation}\ny(x)=\\left(\\frac{\\beta-\\alpha}{b-a}\\right)(x-a)+\\alpha\n\\end{equation}\\]\nwhich is precisely the straight line between \\((a,\\alpha)\\) and \\((b, \\beta)\\). In this case, the function we derive is the same we expect. We have not proved that this \\(y\\) is the minimizing function, but that it is the only extremal value for the functional. Classifying this extremal functions is a bit more complicated and I’ll discuss this another time! Intuitively, it would make since that this function is minimizing, so we’ll just accept this for now.\nIf you need a bit of convincing, you can play with the desmos calculator below! It shows that the straight line gives the minimum arc length when compared to the the family of paths between \\((a, \\alpha)\\) and \\((b, \\beta)\\) described by\n\\[\\begin{equation}\nf_\\epsilon(x)=\\left(\\frac{\\beta-\\alpha}{b-a}\\right)(x-a)+\\alpha + \\underbrace{\\epsilon (x-a)(b-x)}_{\\eta_\\epsilon(x)}.\n\\end{equation}\\]\nThat is, we consider a family of deviations \\(\\eta_\\epsilon\\) from the straight line \\(y\\).\n\n\nThe interesting thing about the calculus of variations is how natural it feels. People are always concerned with the best way to achieve things and most of the time it doesn’t boil down to picking a couple of numbers to solve a single problem, but can depend on the entire form or process or set of possible ways we and nature2 rely on in our attempts to ‘optimize’ the world around us. May that be as simple as proving a straight line is the shortest path between points or as complicated as describing the best fuel distribution to find a reactor of minimal critical mass.\nThat’s all I have to say for now! This is a start to a whole new project of mine (I’m pretending it’s my undergrad thesis since you can’t write one for the math major @ UChicago). It’s a truly beautiful subject matter I’ll be diving into and I’m looking forward to writing more about it!\nThanks for taking the time to read! I’m always open to suggestions about what to write about, so please contact me if you’ve any ideas.\nUntil next time,\n– Math, Magic, & Other Poems.\nMarlin F."
  },
  {
    "objectID": "blog/dive-into-caluclus-of-variations/dive-into-the-calculus-of-variations.html#footnotes",
    "href": "blog/dive-into-caluclus-of-variations/dive-into-the-calculus-of-variations.html#footnotes",
    "title": "A Dive into the Calculus of Variations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’ll discuss this at length and its derivation in the actual monograph, so let’s hope I make more progress on this soon!↩︎\nIf nature can really be said to optimize instead of just following some set of natural laws or principles.↩︎"
  },
  {
    "objectID": "blog/intro-to-probability/intro-to-prob.html",
    "href": "blog/intro-to-probability/intro-to-prob.html",
    "title": "Introduction to Probability I",
    "section": "",
    "text": "Many things in life are seemingly left up to chance. In everyday life, we often hear or say phrases like “That was pretty random.” or “What are the odds of that happening?”, but can you explain what it really means for something to be ‘random’ or ‘up to chance’? Whether you’re a gambler, an anxious student, or just plain sick and tired of the weather recently, you probably want to understand something in your life that seems or feels random.\nProbability is an attempt to make sense of this perceived randomness and provide some laws or axioms for describing random processes so that we can investigate them systematically. In practice, we know that a probability describes the chance that some event will (or will not occur), but this answer alone leaves much to be desired. There’s several fundamental questions we’re left to answer.\nIn my case, the first that comes to mind is “How does one measure a probability?”"
  },
  {
    "objectID": "blog/intro-to-probability/intro-to-prob.html#a-coin-toss-is-the-most-basic-probability-model.",
    "href": "blog/intro-to-probability/intro-to-prob.html#a-coin-toss-is-the-most-basic-probability-model.",
    "title": "Introduction to Probability I",
    "section": "A coin toss is the most basic probability model.",
    "text": "A coin toss is the most basic probability model.\nIn many scenarios, we flip a coin to decide the outcome. For example, if one wants to decide which person goes first in checkers or who has to take out the trash, it’s not uncommon to hear someone say “Let’s flip a coin for it.” Why is this? Is this even fair? What does ‘fair’ even mean when you leave something up to chance like this?\nWell, you might say it’s fair because your intuition tells you that around half of the time we toss a coin it should come up heads and the other half tails. More simply said, we have two possible outcomes \\(\\omega = H\\) and \\(\\omega = T\\) which we think are equally likely (whatever that means).\nTo be precise and consistent, we’ll write these possible outcomes in terms of a set\n\\[\n\\Omega = \\{ H,T \\}.\n\\]\n\n\n\n\nVisualizing \\(\\Omega\\)\n\n\nAs you can see in the most basic coin toss, there are only two outcomes in this experiment. This is a handy model if we need to make a decision between exactly two possible choice, but what will happen if we have 3, 4, or even 1000 possible outcomes.\nLet’s switch to a more complicated example: tossing two coins sequentially. In case of two tosses, we can represent outcomes by looking at the two coin tosses separately. The first toss being \\(\\Omega_1 = \\\\{ H, T \\\\}\\) and the second \\(\\Omega_2 = \\\\{ H, T \\\\}\\). Putting these together, we can see that we have four possible outcomes\n\\[\n\\Omega = \\Omega_1 \\times \\Omega_2 = \\{ HH, HT, TH, TT\\}.\n\\]\n\n\n\nOutcomes \\(\\Omega\\): two coins.\n\n\nThe coin tossing example is extremely neat in that sense that it serves as a nice foundational example for the larger language around probability which we’re trying to develop. As our next step towards that goal, we’ll begin by defining a couple of terms.\nAn outcome \\(\\omega\\) is a single possible result of a given experiment like the coin tosses above.\nThe set of possible outcomes \\(\\Omega\\) is called the sample space.\nAnother important type of objects are events \\(E\\) which describe some subset of outcomes. Events enable us to ask questions about and compute probabilities concerning several different outcomes simultaneously.\nFor example, we might ask the question: “When do we have at least one heads?”. We can then look at individual outcomes \\(\\omega \\in \\Omega\\) which have at least one head and write them as a set\n\\[\nE = \\{ \\omega \\mid \\text{There is at least one $H$} \\} = \\{HH, HT, TH \\}.\n\\]\n\n\n\nSample space \\(\\Omega\\) and an event \\(E\\).\n\n\nOftentimes, we’re interested in the case where an event \\(E\\) does not occur. We call this event the complement of \\(E\\) and write\n\\[\nE^c = \\{w \\not\\in E \\},\n\\]\nwhich is the set of outcomes in our sample space that are not in \\(E\\). In the example above where \\(E\\) is the set where there is at least one \\(H\\),\n\\[\nE^c = \\{ \\omega \\mid \\text{There are exactly zero $H$}\\}  = \\{ TT \\}.\n\\]\nIn the case of two coin tosses, it’s easy enough to count our possible outcomes, but in order to gain a better picture of our coin-tossing experiment in general, we want to understand the specific properties or chances of our outcomes and combinations of them. That is, when discussing events like above, we want to compare and contrast them in various ways. Important tools for this are called the union (\\(\\cup\\)) and intersection (\\(\\cap\\)) of events."
  },
  {
    "objectID": "blog/intro-to-probability/intro-to-prob.html#combining-events",
    "href": "blog/intro-to-probability/intro-to-prob.html#combining-events",
    "title": "Introduction to Probability I",
    "section": "Combining events",
    "text": "Combining events\nThe union of two events \\(A\\) and \\(B\\) is simply their combination i.e. the event either \\(A\\) or \\(B\\) occurs which we write as\n\\[\nA \\cup B = \\{\\omega \\in A \\textbf{ or } \\omega \\in B \\}\n\\]\n\n\n\nVisualizing union (\\(\\cup\\))\n\n\nThe intersection is the overlap between two events. It is the set of outcomes that are in both \\(A\\) and \\(B\\) simultaneously.\n\\[\nA \\cap B = \\{\\omega \\in A \\textbf{ and } \\omega \\in B \\}.\n\\]\n\n\n\nVisualizing intersection (\\(\\cap\\)).\n\n\nLet’s apply these ideas to the coin tossing example.\nConsider the following events. Suppose we flip two fair coins. Let \\(A\\) be the event where exactly two tosses show heads, \\(B\\) where the first coin is heads, and \\(C\\) where the second coin is tails. We begin by writing all of these different events using set notation, so that\n\\[\nA = \\{HH\\}, B = \\{HH, HT\\}, C = \\{HT, TT \\}.\n\\]\nFirst, we’ll find the union of \\(B\\) and \\(C\\). In words, this is the event where either the first coin shows heads or the second is tails. In set notation, this is simply:\n\\[\nB\\cup C = \\{HH, HT \\} \\cup \\{HT, TT \\} = \\{HH, HT, TT \\}.\n\\]\nNext up, let’s find the union of \\(A\\) and \\(B\\). This is the case where either both coins show heads or the first coin shows heads. Writing this out, we see that\n\\[\nA\\cup B = \\{HH \\} \\cup \\{HH, HT\\} = \\{HH, HT\\} = B.\n\\]\nThe event \\(A\\) is a subset of \\(B\\) (\\(A\\subset B\\)). You can think about this as follows: If both coins show heads, then the first coin obviously did. This is obvious, but it helps us make a general observation. Sometimes events will completely contain another. If every outcome in event \\(A\\) is also in event \\(B\\), we say that \\(A\\) is a subset of \\(B\\) and write \\(A\\subset B\\).\n\n\n\nVisualizing subsets (\\(\\subset\\)).\n\n\nLet’s try our hand with the intersection. Starting with the intersection of \\(B\\) and \\(C\\). This is the event where either the first coin shows heads and the second is tails. Using sets, we take the outcomes that \\(B\\) and \\(C\\) have in common\n\\[\nB\\cap C = \\{HH, HT \\} \\cap \\{HT, TT \\} = \\{HT \\}.\n\\]\nIn this case, our intersection only contains a single outcome \\(HT\\) i.e. the case where the first coin showed heads and the second showed tails which is exactly what we were looking for.\nOur choice of \\(B\\) and \\(C\\) lead to us having a very simple intersection, but thee fact remains that some events are disjoint or mutually exclusive, meaning that they do not share any outcomes. Take \\(A\\) and \\(C\\) for example. It is impossible for both coins to show heads and for the second coin to show tails simultaneously. We can write this as\n\\[\nA\\cap C = \\{HH \\} \\cap \\{HT, TT \\} = \\varnothing,\n\\]\nwhere \\(\\varnothing\\) is the empty set containing no elements.\nWith the above ideas in mind, how might we want to describe the probability of various events given that we know that the probability of each coin being heads is \\(\\frac{1}{2}\\)? In the next section, we’ll develop axioms for dealing with problems in probability."
  },
  {
    "objectID": "blog/intro-to-probability/intro-to-prob.html#axioms-of-probability",
    "href": "blog/intro-to-probability/intro-to-prob.html#axioms-of-probability",
    "title": "Introduction to Probability I",
    "section": "Axioms of probability",
    "text": "Axioms of probability\n\\[\n\\newcommand{ \\Prob }{ \\mathbb{P} }\n\\]\nFor this section, I’ll loosely follow Foundations of the Theory of Probability by A.N. Kolmogorov.\nIn a way, we can think of a probability as a way of measuring an event. Much like how people have weights (in both pounds and kilograms), we can think about creating a type of rule or measure \\(\\Prob\\) which weights events and collections of events relative to all possible outcomes. If we’re able to find such a measure, we want it to follow a certain set of rules much like the case of the coin tossing example.\n\n\n\nProbability measures \\(\\Prob\\) take sets to probabilities.\n\n\nFirst up is a simple convention, we would like all probabilities that things happen to be \\(1\\). In the simplest case, an event \\(E\\) can either surely occur so that \\(\\Prob(E) = 1\\) or it cannot \\(\\Prob(E) = 0\\). Therefore, if we look at all possible outcomes simultaneously, we have \\(\\Prob(\\Omega) = 1\\).\nSince all probabilities are positive or zero, we also require that \\(\\Prob(E)\\geq 0\\) for all events \\(E\\).\nLastly, if two events cannot occur simultaneously i.e. \\(A \\cap B = \\varnothing\\), then \\(\\Prob(A \\cup B) = \\Prob(A) + \\Prob(B).\\)\n\n\n\nVisualizing disjoint summations\n\n\nTherefore, our notion of probability must follow these three fundamental rules:\n\\[\n\\begin{align}\n\\Prob(\\Omega) &= 1,\\\\\n\\Prob(E) &\\geq 0 \\text{ for all events } E,\\\\\n\\Prob(A \\cup B) &= \\Prob(A) + \\Prob(B) \\text{ if } A \\cap B = \\varnothing.\n\\end{align}\n\\]\nWe call these three rules our axioms of probability. They define a way of thinking about probability that is both consistent with our intuition and mathematically useful. If you’ve seen probability before, you’ll probably notice that our three axioms are missing some of the basic statements of probability you’ve previously encountered. Most of those statements are the logical consequences of the axioms we’ve outlined. In this sense, our axioms are a way of describing probability with minimal assumptions. Namely, with these axioms alone, we can show that\n\\[\n\\Prob(A) \\leq \\Prob(B) \\text{ if } A \\subset B.\n\\]\nThis also makes sense if we return to the intuition of counting outcomes. If there are more possible outcomes in \\(B\\), then you expect that the chance \\(B\\) occurs is greater than the chance that \\(A\\) occurs as there are now more outcomes to choose from.\n\n\n\nProbability inequality for subsets\n\n\nWhen it comes to taking the union of subsets, there’s a more general formula for subsets which may or may not have overlap.\n\\[\n\\Prob(A \\cup B) = \\Prob(A) + \\Prob(B) - \\Prob( A \\cap B ).\n\\]\n\n\n\nVisualizing the general summation formation for probability of events\n\n\nFormal note: Though it is tempting to say that the set of measurable sets contains every subset, this can sometimes lead us to some strange contradictions. The idea of a permissible collection of measurable sets is formalized by a \\(\\sigma\\)-algebra. You can learn a bit more about this on wikipedia."
  },
  {
    "objectID": "blog/intro-to-probability/intro-to-prob.html#conditional-probability-and-independence",
    "href": "blog/intro-to-probability/intro-to-prob.html#conditional-probability-and-independence",
    "title": "Introduction to Probability I",
    "section": "Conditional probability and independence",
    "text": "Conditional probability and independence\nAs shown previously, the overlap between events is an important consideration when dealing with probability. Due to this overlap, we might want to ask what is the probability of an event \\(A\\) occurring if we have already observed \\(B\\). This is the notion of conditional probability. We can calculate the probability of \\(A\\) given \\(B\\) as\n\\[\n\\Prob(A\\mid B) = \\frac{\\Prob(A\\cap B)}{\\Prob(B)}.\n\\]\nIntuitively, this allows us to investigate how much additional certainty knowledge of event \\(B\\) gives us on event \\(A\\).\nIf knowing \\(B\\) gives no information on whether \\(A\\) occurred, we say that \\(A\\) and \\(B\\) are independent. Mathematically, we write that two events \\(A\\) and \\(B\\) are independent if\n\\[\n\\Prob(A\\cap B) = \\Prob(A) \\cdot \\Prob(B).\n\\]\nThis is equivalent to saying that \\(\\Prob(A \\mid B) = \\Prob(A)\\) and \\(\\Prob(B \\mid A ) = \\Prob(B)\\) which I leave to you as an exercise.\n\n\n\nConditional probability\n\n\nConditional probability is extremely useful because it gives a framework for understanding how certain properties of events may depend on or relate to one another.\nExercise: Prove that \\(A\\) and \\(B\\) are independent if and only if \\(\\Prob(A \\mid B) = \\Prob(A)\\) and \\(\\Prob(B \\mid A ) = \\Prob(B)\\).\nExercise: Prove the Law of Total Probability i.e. that for any two events \\(A\\) and \\(B\\):\n\\[\n\\Prob(A) = \\Prob(A \\mid B) + \\Prob(A \\mid B^c).\n\\]"
  },
  {
    "objectID": "blog/intro-to-probability/intro-to-prob.html#bayes-theorem.",
    "href": "blog/intro-to-probability/intro-to-prob.html#bayes-theorem.",
    "title": "Introduction to Probability I",
    "section": "Bayes’ theorem.",
    "text": "Bayes’ theorem.\nOne particularly important usage of conditional probabilities is Bayes’ theorem. Bayes’ theorem allows us to use the probability of \\(A\\) given \\(B\\) to calculate the probability of \\(B\\) given \\(A\\). This notion is particularly useful when it comes to analyzing how evidence in terms of data affects the likelihood of different probability distributions. Bayes’ theorem is essential for understanding many of the statistical methods used in computational biology and most computational fields which rely on Markov Chain Monte Carlo (MCMC) for fitting models to data.\nMathematically speaking, Bayes’ theorem states that\n\\[\n\\Prob(A\\mid B) = \\frac{\\Prob(B \\mid A) \\cdot \\Prob(A)}{\\Prob(B)}.\n\\]\nIn fact, the proof is simple enough that can derive this directly by writing out the conditional probabilities for \\(A\\) and \\(B\\),\n\\[\n\\begin{align}\n\\Prob(A\\mid B) &= \\frac{\\Prob(A \\cap B)}{\\Prob(B)},\\\\\n\\Prob(B\\mid A) &= \\frac{\\Prob(B \\cap A)}{\\Prob(A)}.\n\\end{align}\n\\]\nSolving for \\(\\Prob(A \\cap B) = \\Prob(B \\cap A)\\) gives us,\n\\[\n\\Prob(A \\mid B) \\cdot \\Prob(B) = \\Prob(A \\cap B) = \\Prob( B \\mid A) \\cdot   \\Prob(A)\n\\], so that\n\\[\n\\Prob(A\\mid B) = \\frac{\\Prob(B \\mid A) \\cdot \\Prob(A)}{\\Prob(B)}.\n\\]\nThis may seem like just another unmotivated equation, but Bayes’ has a useful interpretation when it comes to discrete probabilities. We’ll explore this is the following example."
  },
  {
    "objectID": "blog/intro-to-probability/intro-to-prob.html#application-vampire-hunting-with-bayes",
    "href": "blog/intro-to-probability/intro-to-prob.html#application-vampire-hunting-with-bayes",
    "title": "Introduction to Probability I",
    "section": "Application: Vampire Hunting with Bayes’",
    "text": "Application: Vampire Hunting with Bayes’\nSuppose that we’re the average American and we’re deeply concerned with discovering vampires Given that vampires are a one in a million occurrence, we can use Bayes’ theorem alongside the fact that a limited number of people are allergic to garlic and that all vampires are allergic to garlic to ‘test’ for vampires. Let’s write this in terms of probabilities. According the what we’ve written above, we know three things:\n\\[\n\\begin{align}\n\\Prob(V+) &= \\frac{1}{1000000} = 0.000001,\\\\\n\\Prob(G-\\mid V-) &= \\frac{1}{10000} = 0.00001,\\\\\n\\Prob(G- \\mid V+) &\\approx 1,\n\\end{align}\n\\]\nwhere \\(V+\\) is vampirism positive, \\(V-\\) is human, \\(G-\\) is garlic allergy, \\(G+\\) is garlic tolerant. Plugging this into Bayes’ theorem, we can calculate the probability of being a vampire if you’re allergic to garlic\n\\[\n\\Prob(V+ \\mid G-) = \\frac{\\Prob(G- \\mid V+) \\cdot \\Prob(V+)}{\\Prob(G-)}.\n\\]\nWe have all the values to compute this value except for the probability of being \\(G-\\). We can compute this by taking advantage of the conditional expectation and the Law of Total Probability. That is, we write that\n\\[\n\\Prob(G-) = \\underbrace{  \\Prob(G-\\mid V+) \\cdot \\Prob(V+) }_{\\Prob(\\text{Garlic-allergic vampires})} + \\underbrace{ \\Prob(G-\\mid V-) \\cdot \\Prob(V-) }_{\\Prob(\\text{Garlic-allergic humans})}.\n\\]\nTherefore, the probability that someone is vampire given they’re allergic to garlic\n\\[\n\\Prob(V+ \\mid G-) = \\frac{\\Prob(G- \\mid V+) \\cdot \\Prob(V+)}{ \\Prob(G-\\mid V+) \\cdot \\Prob(V+) + \\Prob(G-\\mid V-) \\cdot \\Prob(V-)}.\n\\]\nThinking of this in terms possible outcomes, this equation shows that probability of being a vampire given a known garlic allergy is the just fraction of garlic-allergic individuals who we expect to be vampires. Since garlic allergy is much more common among vampires, testing first for garlic allergies allows us to have a higher probability of correctly identifying vampires. Computing with the probabilities given above, we find that\n\\[\n\\Prob(V+ \\mid G-) \\approx 0.01 &gt;&gt;&gt; \\Prob(V+) = 0.000001.\n\\]\n\n\n\nIllustrating Bayes\n\n\nTesting for a garlic-allergy means that we’ll be nearly 10,000 times more likely to successfully identify a vampire than if we just randomly tested or sampled the population for vampires. Though vampires are still rare among those with garlic allergies, we’ve managed to increase the probably of them being identified through the garlic test. There is still hope though. By this same logic, the better our ability to test for vampires and their unique characteristics, the easier it becomes to identify them using Bayes’ Theorem. That means that we can use other methods for identifying vampires such as mirrors and our garlic test together to further increase this probability by considering the probabilities jointly. That being said, please don’t go frolicking your neighborhood waving around garlic and hand mirrors, it’s improbable you’ll find anything.\nThis post is the first part of a series where I’ll be going over some of the basic ideas behind the basic probability and stats used in my daily research. Next up, I’ll be writing about probability distributions and estimating parameters for them. For a more thorough introduction to probability, I recommend “Theory of Probability and Random Processes” by Koralov and Sinai.\nThank you for taking the time to read! - Marlin Figgins."
  },
  {
    "objectID": "publications/hct-omicron/hct-omicron.html",
    "href": "publications/hct-omicron/hct-omicron.html",
    "title": "Genomic surveillance of SARS-CoV-2 Omicron variants on a university campus",
    "section": "",
    "text": "Novel variants continue to emerge in the SARS-CoV-2 pandemic. University testing programs may provide timely epidemiologic and genomic surveillance data to inform public health responses. We conducted testing from September 2021 to February 2022 in a university population under vaccination and indoor mask mandates. A total of 3,048 of 24,393 individuals tested positive for SARS-CoV-2 by RT-PCR; whole genome sequencing identified 209 Delta and 1,730 Omicron genomes of the 1,939 total sequenced. Compared to Delta, Omicron had a shorter median serial interval between genetically identical, symptomatic infections within households (2 versus 6 days, P = 0.021). Omicron also demonstrated a greater peak reproductive number (2.4 versus 1.8), and a 1.07 (95% confidence interval: 0.58, 1.57; P &lt; 0.0001) higher mean cycle threshold value. Despite near universal vaccination and stringent mitigation measures, Omicron rapidly displaced the Delta variant to become the predominant viral strain and led to a surge in cases in a university population."
  },
  {
    "objectID": "publications/hct-omicron/hct-omicron.html#abstract",
    "href": "publications/hct-omicron/hct-omicron.html#abstract",
    "title": "Genomic surveillance of SARS-CoV-2 Omicron variants on a university campus",
    "section": "",
    "text": "Novel variants continue to emerge in the SARS-CoV-2 pandemic. University testing programs may provide timely epidemiologic and genomic surveillance data to inform public health responses. We conducted testing from September 2021 to February 2022 in a university population under vaccination and indoor mask mandates. A total of 3,048 of 24,393 individuals tested positive for SARS-CoV-2 by RT-PCR; whole genome sequencing identified 209 Delta and 1,730 Omicron genomes of the 1,939 total sequenced. Compared to Delta, Omicron had a shorter median serial interval between genetically identical, symptomatic infections within households (2 versus 6 days, P = 0.021). Omicron also demonstrated a greater peak reproductive number (2.4 versus 1.8), and a 1.07 (95% confidence interval: 0.58, 1.57; P &lt; 0.0001) higher mean cycle threshold value. Despite near universal vaccination and stringent mitigation measures, Omicron rapidly displaced the Delta variant to become the predominant viral strain and led to a surge in cases in a university population."
  },
  {
    "objectID": "publications/mpox-dynamics/mpox-dynamics.html",
    "href": "publications/mpox-dynamics/mpox-dynamics.html",
    "title": "Underdetected dispersal and extensive local transmission drove the 2022 mpox epidemic",
    "section": "",
    "text": "The World Health Organization declared mpox a public health emergency of international concern in July 2022. To investigate global mpox transmission and population-level changes associated with controlling spread, we built phylogeographic and phylodynamic models to analyze MPXV genomes from five global regions together with air traffic and epidemiological data. Our models reveal community transmission prior to detection, changes in case reporting throughout the epidemic, and a large degree of transmission heterogeneity. We find that viral introductions played a limited role in prolonging spread after initial dissemination, suggesting that travel bans would have had only a minor impact. We find that mpox transmission in North America began declining before more than 10% of high-risk individuals in the USA had vaccine-induced immunity. Our findings highlight the importance of broader routine specimen screening surveillance for emerging infectious diseases and of joint integration of genomic and epidemiological information for early outbreak control."
  },
  {
    "objectID": "publications/mpox-dynamics/mpox-dynamics.html#abstract",
    "href": "publications/mpox-dynamics/mpox-dynamics.html#abstract",
    "title": "Underdetected dispersal and extensive local transmission drove the 2022 mpox epidemic",
    "section": "",
    "text": "The World Health Organization declared mpox a public health emergency of international concern in July 2022. To investigate global mpox transmission and population-level changes associated with controlling spread, we built phylogeographic and phylodynamic models to analyze MPXV genomes from five global regions together with air traffic and epidemiological data. Our models reveal community transmission prior to detection, changes in case reporting throughout the epidemic, and a large degree of transmission heterogeneity. We find that viral introductions played a limited role in prolonging spread after initial dissemination, suggesting that travel bans would have had only a minor impact. We find that mpox transmission in North America began declining before more than 10% of high-risk individuals in the USA had vaccine-induced immunity. Our findings highlight the importance of broader routine specimen screening surveillance for emerging infectious diseases and of joint integration of genomic and epidemiological information for early outbreak control."
  },
  {
    "objectID": "publications/ncov-orf8-ko/ncov-orf8-ko.html",
    "href": "publications/ncov-orf8-ko/ncov-orf8-ko.html",
    "title": "Positive selection underlies repeated knockout of ORF8 in SARS-CoV-2 evolution",
    "section": "",
    "text": "Knockout of the ORF8 protein has repeatedly spread through the global viral population during SARS-CoV-2 evolution. Here we use both regional and global pathogen sequencing to explore the selection pressures underlying its loss. In Washington State, we identified transmission clusters with ORF8 knockout throughout SARS-CoV-2 evolution, not just on novel, high fitness viral backbones. Indeed, ORF8 is truncated more frequently and knockouts circulate for longer than for any other gene. Using a global phylogeny, we find evidence of positive selection to explain this phenomenon: nonsense mutations resulting in shortened protein products occur more frequently and are associated with faster clade growth rates than synonymous mutations in ORF8. Loss of ORF8 is also associated with reduced clinical severity, highlighting the diverse clinical impacts of SARS-CoV-2 evolution."
  },
  {
    "objectID": "publications/ncov-orf8-ko/ncov-orf8-ko.html#abstract",
    "href": "publications/ncov-orf8-ko/ncov-orf8-ko.html#abstract",
    "title": "Positive selection underlies repeated knockout of ORF8 in SARS-CoV-2 evolution",
    "section": "",
    "text": "Knockout of the ORF8 protein has repeatedly spread through the global viral population during SARS-CoV-2 evolution. Here we use both regional and global pathogen sequencing to explore the selection pressures underlying its loss. In Washington State, we identified transmission clusters with ORF8 knockout throughout SARS-CoV-2 evolution, not just on novel, high fitness viral backbones. Indeed, ORF8 is truncated more frequently and knockouts circulate for longer than for any other gene. Using a global phylogeny, we find evidence of positive selection to explain this phenomenon: nonsense mutations resulting in shortened protein products occur more frequently and are associated with faster clade growth rates than synonymous mutations in ORF8. Loss of ORF8 is also associated with reduced clinical severity, highlighting the diverse clinical impacts of SARS-CoV-2 evolution."
  },
  {
    "objectID": "publications/rt-from-frequency-dynamics/rt-from-frequency-dynamics.html",
    "href": "publications/rt-from-frequency-dynamics/rt-from-frequency-dynamics.html",
    "title": "SARS-CoV-2 variant dynamics across US states show consistent differences in effective reproduction numbers",
    "section": "",
    "text": "Accurately estimating relative transmission rates of SARS-CoV-2 Variant of Concern and Variant of Interest viruses remains a scientific and public health priority. Recent studies have used the sample proportions of different variants from sequence data to describe variant frequency dynamics and relative transmission rates, but frequencies alone cannot capture the rich epidemiological behavior of SARS-CoV-2. Here, we extend methods for inferring the effective reproduction number of an epidemic using confirmed case data to jointly estimate variant-specific effective reproduction numbers and frequencies of co-circulating variants using case data and genetic sequences across states in the US from January to October 2021. Our method can be used to infer structured relationships between effective reproduction numbers across time series allowing us to estimate fixed variant-specific growth advantages. We use this model to estimate the effective reproduction number of SARS-CoV-2 Variants of Concern and Variants of Interest in the United States and estimate consistent growth advantages of particular variants across different locations."
  },
  {
    "objectID": "publications/rt-from-frequency-dynamics/rt-from-frequency-dynamics.html#abstract",
    "href": "publications/rt-from-frequency-dynamics/rt-from-frequency-dynamics.html#abstract",
    "title": "SARS-CoV-2 variant dynamics across US states show consistent differences in effective reproduction numbers",
    "section": "",
    "text": "Accurately estimating relative transmission rates of SARS-CoV-2 Variant of Concern and Variant of Interest viruses remains a scientific and public health priority. Recent studies have used the sample proportions of different variants from sequence data to describe variant frequency dynamics and relative transmission rates, but frequencies alone cannot capture the rich epidemiological behavior of SARS-CoV-2. Here, we extend methods for inferring the effective reproduction number of an epidemic using confirmed case data to jointly estimate variant-specific effective reproduction numbers and frequencies of co-circulating variants using case data and genetic sequences across states in the US from January to October 2021. Our method can be used to infer structured relationships between effective reproduction numbers across time series allowing us to estimate fixed variant-specific growth advantages. We use this model to estimate the effective reproduction number of SARS-CoV-2 Variants of Concern and Variants of Interest in the United States and estimate consistent growth advantages of particular variants across different locations."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "This will be a page where I keep my slides\n\n\n\n\n\n\n\n\nFrom Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness\n\n\n\n\n\n\nMarlin Figgins\n\n\nMar 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEpidemics, evolution, and too much math\n\n\n\n\n\n\nMarlin Figgins\n\n\nSep 8, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#evolution-and-selection-in-sars-cov-2",
    "href": "talks/SFAMeeting/SFA_meeting.html#evolution-and-selection-in-sars-cov-2",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Evolution and selection in SARS-CoV-2",
    "text": "Evolution and selection in SARS-CoV-2\n\nAll-time phylogenetic tree for SARS-CoV-2 from Nextstrain"
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#why-study-selection",
    "href": "talks/SFAMeeting/SFA_meeting.html#why-study-selection",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Why study selection?",
    "text": "Why study selection?\nWe are interested in the selection in SARS-CoV-2 for two main reasons.\n\nAwareness: Quantifying variant advantages to understand the viral population right now or in short-term\nPreparation: Forecasting variant advantage to predict viral diversity in the future\n\nInstead of focusing on sequence-based methods which may be computationally infeasible with large numbers of sequences, we use coarser labels for estimating fitness of groups.\nThese labels can be at granular as you choose such as:\n\nhaplotype,\nPango lineage,\nNextstrain clade."
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#selection-and-evolution",
    "href": "talks/SFAMeeting/SFA_meeting.html#selection-and-evolution",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Selection and Evolution",
    "text": "Selection and Evolution\n\n\n\n\nSelection is the process by which individuals have higher fitness in certain environments\nEvolution is the change in the genetic composition of the population over time due to selection and heritable variation\nRelative fitness is the relative capacity for individuals to reproduce in a population"
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#illustrating-selection",
    "href": "talks/SFAMeeting/SFA_meeting.html#illustrating-selection",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Illustrating selection",
    "text": "Illustrating selection\n\n\n\n\n\n\n\n\n\n\n\nAn early mutation that takes us from green infections to purple which cause more secondary infection.\nIn this case, the purple is selected for."
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#quantifying-selection-and-relative-fitness",
    "href": "talks/SFAMeeting/SFA_meeting.html#quantifying-selection-and-relative-fitness",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Quantifying selection and relative fitness",
    "text": "Quantifying selection and relative fitness\n\n\n\n\nOften, we’re interested in not just the presence of selection but its magnitude.\nWhen quantifying selection, you’ll often see selective coefficient or relative fitness discussed.\nRelative fitness is the difference in the growth rates of two variants i.e. the relative capacity for individuals to reproduce in a population\n\n\n\n\n\n\n  \n\n\n\n\n\nRemark: For those familiar with the selective coefficient \\(s\\), the relative fitness \\(\\lambda\\) is given by \\((1 + s) = \\exp(\\lambda)\\)"
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#generating-the-data",
    "href": "talks/SFAMeeting/SFA_meeting.html#generating-the-data",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Generating the data",
    "text": "Generating the data\nGenerally, we want to take sequences and classify them into variant groups such as Nextstrain clade or Pango lineage and then count their occurance per day or week. This will give us data of the form:\n\n\n\n\n\n\n\n\n\n\nvariant\nsequences\ndate\n\n\n\n\n0\nwildtype\n1\n2022-01-01\n\n\n1\nwildtype\n12\n2022-01-02\n\n\n2\nwildtype\n17\n2022-01-03\n\n\n3\nvariant\n1\n2022-01-03\n\n\n4\nwildtype\n11\n2022-01-04\n\n\n5\nvariant\n1\n2022-01-04\n\n\n6\nwildtype\n15\n2022-01-05\n\n\n\n\n\n\n\n\nData of this form is automatically generated at the level of US state and country in forecasts-ncov."
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#generating-sequence-counts",
    "href": "talks/SFAMeeting/SFA_meeting.html#generating-sequence-counts",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Generating sequence counts",
    "text": "Generating sequence counts"
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#estimating-relative-fitness-of-variants",
    "href": "talks/SFAMeeting/SFA_meeting.html#estimating-relative-fitness-of-variants",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Estimating relative fitness of variants",
    "text": "Estimating relative fitness of variants\n\n\n\nIf we’re interested in estimating the relative fitness of variants from sequence counts, we need to model the change in frequencies over time.\nWe want to turn these into frequency and estimate the relative fitness we discussed before using the following equation:\n\\[\np_{v}(t) = \\frac{p_{v}(0)\\exp(\\lambda_v t)}{\\sum_{u} p_{u}(0)\\exp(\\lambda_u t)}.\n\\]\nThis means that we’re estimating the following parameters:\n\n\\(p_{v}(t)\\) is the frequency of variant \\(v\\) at time \\(t\\).\n\\(p_{v}(0)\\) is the initial frequency of variant \\(v\\).\n\\(\\lambda_{v}\\) is the relative fitness of variant \\(v\\).\n\nWe’ll often work with the growth advantage \\(\\Delta_v = \\exp(\\lambda_{v} \\tau)\\) where \\(\\tau\\) is the generation time."
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#estimating-relative-fitness-using-evofr",
    "href": "talks/SFAMeeting/SFA_meeting.html#estimating-relative-fitness-using-evofr",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Estimating relative fitness using evofr",
    "text": "Estimating relative fitness using evofr\n\n\nWe’ve developed a package called evofr that implements these models.\n\nimport evofr as ef\n\n# Loading data as VariantFrequencies\nvariant_counts = pd.read_csv(\"./data/variant_counts.tsv\", sep=\"\\t\")\ndata = ef.VariantFrequencies(\n    raw_seq = variant_counts,\n    pivot = \"wildtype\"\n)\n\n# Define model: tau is generation time\nmodel = ef.MultinomialLogisticRegression(tau=1)\n\n# Do inference using NUTS MCMC\ninference_method = ef.InferNUTS(\n    num_samples=500, \n    num_warmup=100\n    )\n\nposterior = inference_method.fit(model, data)"
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#estimating-relative-fitness-with-evofr",
    "href": "talks/SFAMeeting/SFA_meeting.html#estimating-relative-fitness-with-evofr",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Estimating relative fitness with evofr",
    "text": "Estimating relative fitness with evofr\n\n\nVisualizing posterior frequencies:\n\nfrom evofr.plotting import FrequencyPlot\nFrequencyPlot(posterior, color_map=colors).plot()\n\n\n\nVisualizing growth advantages:\n\nfrom evofr.plotting import GrowthAdvantagePlot\nGrowthAdvantagePlot(posterior, color_map=colors).plot()"
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#estimating-relative-fitness-across-multiple-locations",
    "href": "talks/SFAMeeting/SFA_meeting.html#estimating-relative-fitness-across-multiple-locations",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Estimating relative fitness across multiple locations",
    "text": "Estimating relative fitness across multiple locations\n\n\nThis idea is pretty simple, we’re just saying that we think the relative fitnesses should be similar across geographies: \\[\n\\lambda_{v, g} \\sim \\text{Normal}(\\bar{\\lambda}_v, \\sigma_{v})\n\\]\n\\(\\lambda_{v,g}\\) is the relative fitness of variant \\(v\\) in geography \\(g\\).\n\\(\\bar{\\lambda}\\) is the mean relative fitness of variant \\(v\\) across geographies.\n\\(\\sigma_v\\) is the standard deviation in the relative fitness of \\(v\\).\nThis is called partial pooling and it allows us to share information about the relative fitness across locations.\nLow-data locations can receive information about variants that they haven’t seen yet or are below detectable levels.\n\n\nSource: Carpenter, Gabry, Goodrich."
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#using-hierarchical-models-in-evofr",
    "href": "talks/SFAMeeting/SFA_meeting.html#using-hierarchical-models-in-evofr",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Using Hierarchical models in Evofr",
    "text": "Using Hierarchical models in Evofr\n\nimport evofr as ef\n\n# Loading data as HierFrequencies\n# Your data .csv should now have a location column\nvariant_counts = pd.read_csv(\"./data/variant_counts.tsv\", sep=\"\\t\")\ndata = ef.HierFrequencies(\n    raw_seq = variant_counts,\n    pivot = \"wildtype\",\n    group=\"location\"\n)\n\nmodel = ef.HierMLR(tau=1, pool_scale=1e-4)\n\n# Do inference using NUTS MCMC\ninference_method = ef.InferNUTS(\n    num_samples=500, \n    num_warmup=100\n    )\n\nposterior = inference_method.fit(model, data)"
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#automating-sars-cov-2-variant-frequency-forecasts-forecasts-ncov",
    "href": "talks/SFAMeeting/SFA_meeting.html#automating-sars-cov-2-variant-frequency-forecasts-forecasts-ncov",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Automating SARS-CoV-2 variant frequency forecasts: forecasts-ncov",
    "text": "Automating SARS-CoV-2 variant frequency forecasts: forecasts-ncov\n\n\n\nWe’ve developed a pipeline for:\n\nprovisioning these sequence count data sets from both GISAID and open data,\nrunning the hierarchical MLR models on these data sets,\nand visualizing their results at https://nextstrain.org/sars-cov-2/forecasts/.\n\nThis work was done with Jover Lee, James Hadfield, and Trevor Bedford."
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#nextstrain-sars-cov-2-forecasts-link",
    "href": "talks/SFAMeeting/SFA_meeting.html#nextstrain-sars-cov-2-forecasts-link",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Nextstrain SARS-CoV-2 forecasts (link)",
    "text": "Nextstrain SARS-CoV-2 forecasts (link)"
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#seattle-flu-alliance-dashboard-link",
    "href": "talks/SFAMeeting/SFA_meeting.html#seattle-flu-alliance-dashboard-link",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Seattle Flu Alliance Dashboard (link)",
    "text": "Seattle Flu Alliance Dashboard (link)\n\nAmanda Perofsky has led the charge of applying this kind of pipeline to Seattle Flu Alliance data."
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#seattle-flu-alliance-dashboard-link-1",
    "href": "talks/SFAMeeting/SFA_meeting.html#seattle-flu-alliance-dashboard-link-1",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Seattle Flu Alliance Dashboard (link)",
    "text": "Seattle Flu Alliance Dashboard (link)"
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#forecasting-variant-frequencies-is-complicated",
    "href": "talks/SFAMeeting/SFA_meeting.html#forecasting-variant-frequencies-is-complicated",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Forecasting variant frequencies is complicated!",
    "text": "Forecasting variant frequencies is complicated!\n\nAs we continue to develop these kinds of models, it’s essential to think about what they can do and when they work best!"
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#mimicking-real-time-forecast-environments",
    "href": "talks/SFAMeeting/SFA_meeting.html#mimicking-real-time-forecast-environments",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Mimicking real-time forecast environments",
    "text": "Mimicking real-time forecast environments\n\nAs we continue to develop these kinds of models, it’s essential to think about what they can do and when they work best!"
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#evaluating-forecasts-1",
    "href": "talks/SFAMeeting/SFA_meeting.html#evaluating-forecasts-1",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Evaluating forecasts",
    "text": "Evaluating forecasts\n\nWe find that in general existing frequency dynamic models work well for short-term forecasts."
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#conclusions",
    "href": "talks/SFAMeeting/SFA_meeting.html#conclusions",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Conclusions",
    "text": "Conclusions\n\nEstimating variant fitness with coarse-grain sequence counts can be useful for monitoring the rise of variants\nWe’ve implemented the methodology for doing this and sharing information across geographies in a software package evofr.\nThere’s a lot of room to expand and adapt these methods to different pathogens as well as to extend them to incorporate new data sources .etc"
  },
  {
    "objectID": "talks/SFAMeeting/SFA_meeting.html#questions",
    "href": "talks/SFAMeeting/SFA_meeting.html#questions",
    "title": "From Sequencing to Surveillance: Estimating SARS-CoV-2 variant fitness",
    "section": "Questions?",
    "text": "Questions?"
  }
]