[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Methods for epidemic-evolutionary forecasting",
    "section": "",
    "text": "Population dynamics and evolution can occur on similar time scales when mutation occurs rapidly along replication. The case of transmission of infectious diseases is an especially interesting example of this.\nAs infected individuals transmit to one another, pathogens can accumulate mutations which can change the potential for escape from immune responses, increased transmissibility, and/or can lead to differential fitness for different genetic variants.\n\n\n\n\nTransmission tree for a two variant system. Dashed lines denote new mutations.\n\n\n\n\n\n\n\n\nIn order to quantify the fitness of genetic variants at transmission, we (Figgins and Bedford 2022) developed a method from learning effective reproduction numbers and variant relative growth advantages using sequence and case count data. We apply this method to several US states to learn about the growth advantages for SARS-CoV-2 variants of concern."
  },
  {
    "objectID": "research.html#improving-evolutionary-forecasts",
    "href": "research.html#improving-evolutionary-forecasts",
    "title": "Methods for epidemic-evolutionary forecasting",
    "section": "Improving evolutionary forecasts",
    "text": "Improving evolutionary forecasts\n\nForecasting evolution can be useful for improving vaccination\nAs viruses mutate, vaccine effectiveness can wane due to immune mismatch between circulating virus strains and strains used in vaccines. For viruses like SARS-CoV-2 and influenza understanding how viruses differ in their fitness can help us to better forecast their evolution. This allows us better understand the process of evolution itself, but also has practical applications in improving vaccines.\nCurrently, I am interested in tying in mechanistic models of transmission, immunity, and recovery to improve methods for evolutionary forecasting. The hope is that by including more information about these processes, we can gain better estimates of relative fitness and better forecasts for virus populations."
  },
  {
    "objectID": "research.html#tool-building-enables-others-to-learn-and-contribute",
    "href": "research.html#tool-building-enables-others-to-learn-and-contribute",
    "title": "Methods for epidemic-evolutionary forecasting",
    "section": "Tool-building enables others to learn and contribute",
    "text": "Tool-building enables others to learn and contribute\n\nWe build tools to make science more accessible and collaborative\nIn general, I am motivated by the desire to build tools to help folks answer questions they have about epidemics and how pathogens evolve. For this purpose, much of my time is dedicated to making sure the models and methods I develop are available freely and well documented, so that others can learn about them and make their own contributions to science.\n\n\nScience communication and education comes with the territory\nIn my opinion, science is the process of asking questions, answering questions, and presenting information. Further, I believe an important part of the process of doing science or really any kind of inquiry is enabling others to take action. To me, this primarily means creating resources for others to use whether that be educational material, a new software package, or any other tool."
  },
  {
    "objectID": "publications/hct-omicron/hct-omicron.html",
    "href": "publications/hct-omicron/hct-omicron.html",
    "title": "Mapping the emergence of SARS-CoV-2 Omicron variants on a university campus",
    "section": "",
    "text": "Novel variants continue to emerge in the SARS-CoV-2 pandemic. University testing programs may provide timely epidemiologic and genomic surveillance data to inform public health responses. We conducted testing from September 2021 to February 2022 in a university population under vaccination and indoor mask mandates. A total of 3,048 of 24,393 individuals tested positive for SARS-CoV-2 by RT-PCR; whole genome sequencing identified 209 Delta and 1,730 Omicron genomes of the 1,939 total sequenced. Compared to Delta, Omicron had a shorter median serial interval between genetically identical, symptomatic infections within households (2 versus 6 days, P=0.021). Omicron also demonstrated a greater peak reproductive number (2.4 versus 1.8) and a 1.07 (95% confidence interval: 0.58, 1.57; P<0.0001) higher mean cycle threshold value. Despite near universal vaccination and stringent mitigation measures, Omicron rapidly displaced the Delta variant to become the predominant viral strain and led to a surge in cases in a university population."
  },
  {
    "objectID": "publications/rt-from-frequency-dynamics/rt-from-frequency-dynamics.html",
    "href": "publications/rt-from-frequency-dynamics/rt-from-frequency-dynamics.html",
    "title": "SARS-CoV-2 variant dynamics across US states show consistent differences in effective reproduction numbers",
    "section": "",
    "text": "Accurately estimating relative transmission rates of SARS-CoV-2 Variant of Concern and Variant of Interest viruses remains a scientific and public health priority. Recent studies have used the sample proportions of different variants from sequence data to describe variant frequency dynamics and relative transmission rates, but frequencies alone cannot capture the rich epidemiological behavior of SARS-CoV-2. Here, we extend methods for inferring the effective reproduction number of an epidemic using confirmed case data to jointly estimate variant-specific effective reproduction numbers and frequencies of co-circulating variants using case data and genetic sequences across states in the US from January to October 2021. Our method can be used to infer structured relationships between effective reproduction numbers across time series allowing us to estimate fixed variant-specific growth advantages. We use this model to estimate the effective reproduction number of SARS-CoV-2 Variants of Concern and Variants of Interest in the United States and estimate consistent growth advantages of particular variants across different locations."
  },
  {
    "objectID": "publications/publications.html",
    "href": "publications/publications.html",
    "title": "Publications",
    "section": "",
    "text": "Weil et al. 2022. medRxiv.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiggins M, Bedford T. 2021. medRxiv\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#who-am-i",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#who-am-i",
    "title": "Epidemics, evolution, and too much math",
    "section": "Who am I?",
    "text": "Who am I?\n\nUChicago alumnus (BS, Mathematics 2019)\nCurrent PhD student in Applied Math at Fred Hutch and the University of Washington in the Bedford lab.\nNow, I study transmission and evolution of pathogens like influenza and SARS-CoV-2."
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#epidemics-are-often-observed-with-counts",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#epidemics-are-often-observed-with-counts",
    "title": "Epidemics, evolution, and too much math",
    "section": "Epidemics are often observed with counts",
    "text": "Epidemics are often observed with counts"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#we-use-r_t-to-infer-the-direction-of-epidemics",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#we-use-r_t-to-infer-the-direction-of-epidemics",
    "title": "Epidemics, evolution, and too much math",
    "section": "We use \\(R_{t}\\) to infer the direction of epidemics",
    "text": "We use \\(R_{t}\\) to infer the direction of epidemics\n{.fig-align=“center”}"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#pathogens-have-an-evolutionary-history",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#pathogens-have-an-evolutionary-history",
    "title": "Epidemics, evolution, and too much math",
    "section": "Pathogens have an evolutionary history",
    "text": "Pathogens have an evolutionary history"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#mutation-can-lead-to-transmission-differences",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#mutation-can-lead-to-transmission-differences",
    "title": "Epidemics, evolution, and too much math",
    "section": "Mutation can lead to transmission differences",
    "text": "Mutation can lead to transmission differences"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#variant-differences-may-not-show-in-cases",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#variant-differences-may-not-show-in-cases",
    "title": "Epidemics, evolution, and too much math",
    "section": "Variant differences may not show in cases",
    "text": "Variant differences may not show in cases"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#variant-frequencies-allow-us-to-monitor-change",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#variant-frequencies-allow-us-to-monitor-change",
    "title": "Epidemics, evolution, and too much math",
    "section": "Variant frequencies allow us to monitor change",
    "text": "Variant frequencies allow us to monitor change"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#turning-frequencies-into-growth-advantages",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#turning-frequencies-into-growth-advantages",
    "title": "Epidemics, evolution, and too much math",
    "section": "Turning frequencies into growth advantages",
    "text": "Turning frequencies into growth advantages"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#transmission-models-with-various-strains",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#transmission-models-with-various-strains",
    "title": "Epidemics, evolution, and too much math",
    "section": "Transmission models with various strains",
    "text": "Transmission models with various strains\n{.fig-align=“center”}"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#developing-tools-for-analyzing-epidemics",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#developing-tools-for-analyzing-epidemics",
    "title": "Epidemics, evolution, and too much math",
    "section": "Developing tools for analyzing epidemics",
    "text": "Developing tools for analyzing epidemics\n\nHow did we go from a one-off model to a tool that we can continually use?\nMy research is focused on building tools and toolkits for analyzing evolution of pathogens.\nThis includes both software (like that used to make today’s figures) and mathematical machinery that teaches how and why growth advantages may appear between pathogen variants."
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#takeaways",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#takeaways",
    "title": "Epidemics, evolution, and too much math",
    "section": "Takeaways",
    "text": "Takeaways\n\nUnderstanding epidemics is complicated by feedback loop between evolution and transmission\nMathematics and statistics are tools for analyzing data with these tools in mind.\nOnce we have an idea in mind, we can build re-usable and customizable tools for analyzing new data.\nThis allows us to make many forecasts as we receive new data and opens the door for evolutionary forecasting"
  },
  {
    "objectID": "talks/FredHutchCareerExploration/FHCareerExploration.html#questions",
    "href": "talks/FredHutchCareerExploration/FHCareerExploration.html#questions",
    "title": "Epidemics, evolution, and too much math",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "TODO.html#figure-out-how-to-put-my-publications-in-a-pretty-way",
    "href": "TODO.html#figure-out-how-to-put-my-publications-in-a-pretty-way",
    "title": "Marlin Figgins",
    "section": "Figure out how to put my publications in a pretty way",
    "text": "Figure out how to put my publications in a pretty way\n\nResearch page: What are my general interests?"
  },
  {
    "objectID": "TODO.html#design-my-homepage",
    "href": "TODO.html#design-my-homepage",
    "title": "Marlin Figgins",
    "section": "Design my homepage",
    "text": "Design my homepage"
  },
  {
    "objectID": "TODO.html#figure-out-some-content-to-put-on-here.",
    "href": "TODO.html#figure-out-some-content-to-put-on-here.",
    "title": "Marlin Figgins",
    "section": "Figure out some content to put on here.",
    "text": "Figure out some content to put on here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marlin Figgins",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     Github\n  \n  \n    \n     CV\n  \n\n  \n  \n\n\nMy name is Marlin Figgins. I am a third-year PhD candidate in the University of Washington’s Department of Applied Mathematics as a Boeing, ARCS, and NSF GRFP fellow.\nI currently develop methods for modeling, Bayesian inference, and forecasting of epidemic and evolutionary dynamics of respiratory viruses in the Bedford lab at the Fred Hutchinson Cancer Research Center. You can learn more about this work on the Research page.\nFor further details on my professional life, you can view and download my CV in PDF format on this page.\n\n\nMy personal email is marlinfiggins [at] gmail [dot] com, but I can also be reached mfiggins [at] uw [dot] edu."
  },
  {
    "objectID": "index.html#most-recent-blog-posts",
    "href": "index.html#most-recent-blog-posts",
    "title": "Marlin Figgins",
    "section": "Most recent blog posts",
    "text": "Most recent blog posts\n\n\n\n\n\n\n\n\n\n\nIntroduction to Probability I\n\n\n\nAug 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSocial Distancing and Me\n\n\n\nMar 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChaos: What it is and where to find it\n\n\n\nJul 12, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/chaos/chaos-what-is-it-where-to-find-it.html",
    "href": "blog/chaos/chaos-what-is-it-where-to-find-it.html",
    "title": "Chaos: What it is and where to find it",
    "section": "",
    "text": "If we’re interested in taking a first step towards chaos, the usual starting example is the logistic map. A thousand blog posts have been written about this map and chaos in general. Instead of throwing the equation in your face, I’ll try and show this chaos begins to appear in a very simple system. First, let’s start with the simplest possible model for population size. \\(\\newcommand{\\abs}[1]{ \\left| #1 \\right| }\\)"
  },
  {
    "objectID": "blog/chaos/chaos-what-is-it-where-to-find-it.html#the-exponential-map",
    "href": "blog/chaos/chaos-what-is-it-where-to-find-it.html#the-exponential-map",
    "title": "Chaos: What it is and where to find it",
    "section": "The Exponential Map",
    "text": "The Exponential Map\nWhat if every person had on average 1.2 kids every generation? Starting with \\(x_0\\) people, the new population would be \\(1.2 x_0\\) the next generation. After two generations, it’d be \\((1.2)^2 x_0\\) and it’d continue like this for the following generations. We have some growth every generation and it multiplies our current population over and over again. If we want to be more general about this, we can say that the population has a growth parameter \\(\\lambda\\). Then we’d write our population growth each generation as a function\n\\[\n\\begin{equation}\nf(x) = \\lambda x.\n\\end{equation}\n\\]\nIn general, this function is called the exponential map and tells us how big the next generation’s population will be given the population grows by a factor of \\(\\lambda\\). If we keep applying it over and over, we can get the population size several generations in the future. Mathematically, we’d write this as a dynamical system. If we have a starting value or /initial condition \\(x_0\\). Therefore, the population after 1 generation would be \\(f^1(x_0) = \\lambda x_0\\), after two it would be \\(f^2(x_0) = \\lambda^2x_0\\). For simplicity’s sake, we’d say that after \\(n\\) generations the population is\n\\[\n\\begin{equation}\nf^n(x_0)=\\lambda^nx_0.\n\\end{equation}\n\\]\nWe can describe the dynamics of the exponential map by looking at all the population values for each generation\n\\[\\begin{equation}\nx_0, f^1(x_0), f^2(x_0), f^3(x_0), \\dotsc\n\\end{equation}\n\\]\nFrom this, we can begin to see that the dynamics will depend on the value of \\(\\lambda\\) we pick. When dealing with the exponential map, we can have two kinds of outcomes. The population will either die out (\\(0<\\lambda < 1\\)) eventually or it will skyrocket and go to infinity (\\(\\lambda >1\\)). If \\(\\lambda < 1\\), then \\(\\lambda \\cdot \\lambda < \\lambda \\cdot 1\\), so the population shrinks and the population will die out over time. On the other hand, if \\(\\lambda > 1\\), then \\(\\lambda \\cdot \\lambda > \\lambda \\cdot 1\\) so the population grows and continues to grow every generation.\nIn the case we were discussing before, \\(\\lambda\\) would be 1.2 kids per person. Each generation, the population would steadily increase and after 20 generations, we’d have over 30,00 people. After 100, we’d have nearly 80 billion!\n\n\n\nGeneration:\n1\n2\n3\n4\n5\n6\n\n\n\n\nPopulation:\n1000\n1200\n1440\n1720\n2070\n2490\n\n\n\n\n\n\n\nPlotting the exponential map.\n\n\nThis isn’t very realistic because it assumes the population can get infinitely large. When it comes to human population, this probably isn’t realistic. Population growth has to begin to slow down at some point. In this case, there must be some limiting factor to our population growth.\nInstead of doing this by hand each time, we could also use a compute program to simulate these dynamics. We can use the following code to simulate these dynamics. Here’s an example program I wrote below.\ndef exponential(x_0 = 100, lambd = 1, max_n = 10):\n   x = []\n   x = np.append(x, x_0)\n\n   for n in range(max_n - 1):\n       x = np.append(x, lambd*x[n])\n\n   return x"
  },
  {
    "objectID": "blog/chaos/chaos-what-is-it-where-to-find-it.html#logistic-map",
    "href": "blog/chaos/chaos-what-is-it-where-to-find-it.html#logistic-map",
    "title": "Chaos: What it is and where to find it",
    "section": "Logistic Map",
    "text": "Logistic Map\nWhat happens to our model if we assume that resources are somehow limited? As more people appear, it’ll become harder for everyone to meet their individual needs to survive . In order to model this, we need the population change to depend not only on a growth rate, but also how close we are to some theoretical maximum population size. If we say \\(x\\) is the current fraction of this maximum population size, we might change our growth rate from \\(\\lambda\\) to \\(\\lambda (1-x)\\) to allow for the growth rate to decrease as the population increases. Therefore, our model would become\n\\[\n\\begin{equation}\nf(x) = \\lambda(1-x) x.\n\\end{equation}\n\\]\nThis is called the logistic map. Strictly speaking, it is a family of maps depending on different values of \\(\\lambda\\). In order to see what happens to the population each generation, we want to analyze the dynamics like in the last example. In the last example, we were able to write a formula for the dynamics and saw the population would either blow up to infinity or die out eventually. This time around we can’t easily write an explicit equation for the dynamics. One usual tool in cases like these where we can’t easily write down a solution is simulation. We can rely on code to help us visualize the population dynamics over time. Here’s the program I use to the simulate the logistic map in the figures below.\n## Logistic Map\ndef logistic_map(x_0 = 0.4, lambd = 1.2, max_n = 500):\n    x = []\n    x = np.append(x, x_0)\n\n    for n in range(max_n - 1):\n        x_new = lambd*x[n]*(1-x[n])\n        x = np.append(x, x_new)\n    return(x)\nDespite having the tools to simulate this, let’s try to understand this at an intuitive level first since all good code needs a sanity check. We made this model to account for limits on a population and picked a decreasing growth rate, so you might expect that the dynamics will reflect this and keep the population in check. Even though this works on an intuitive level, we should still try and observe it mathematically. Following this intuition, the first thing we should check is whether our population will stabilize as we expect. Much like in the case of the exponential map, this all depends on the value of the growth parameter \\(\\lambda\\).\nIt’s actually not that hard to prove that there is some point of stability where the population will stay the same from year to year. In mathematical terms, this is called a fixed point. From our point of view, we say the population \\(x_\\star\\) is fixed if \\(f(x_\\star) = x_\\star\\).\nProposition: Every continuous function \\(f\\) takes points from \\([0,1]\\) to \\([0,1]\\) has a fixed point.\nProof. Let’s define a new function \\(D(x) = f(x) - x\\). Since \\(f(0)\\) must be in \\([0,1]\\), it must be greater than or equal to 0. Therefore, we know \\[D(0) = f(0) - 0 \\geq 0.\\] Similarly, \\(f(1)\\) is in \\([0,1]\\), so we know that \\(D(1)<0\\). Since \\(f\\) is continuous, the intermediate value theorem guarantees us that there is a fixed point \\(x_\\star\\).\nIn order for this to hold for the logistic map, we need to make sure that \\(f(x)\\) stays between 0 and 1 which means that \\(\\lambda\\) must be between 0 and 4. Under these assumptions, we can solve for our fixed point(s) directly. One obvious fixed point is \\(x_\\star = 0\\), but let’s see if there are any others. If \\(x_\\star\\) is a non-zero fixed point of \\(f(x)\\), then we need \\(\\lambda(1-x)x = x\\). Dividing by \\(x\\), we get \\(\\lambda(1-x) = 1\\). Multiplying this out, we get \\(\\lambda - 1 = \\lambda x\\) which gives shows that our equilibrium value must be\n\\[\n\\begin{equation}\nx_\\star = 1 - \\frac{1}{\\lambda}.\n\\end{equation}\n\\]\nNotice that in order for this fixed point to be greater than 0, we need \\(\\lambda\\) to be greater than 1. This makes sense since otherwise we can expect the population to die out like in the exponential case.\nWe know it’s possible for our population to stabilize, but when will it? We can use the derivative of this function to help us discern whether this point is attracting i.e. whether initial conditions near it will approach this fixed point.\nProposition: If \\(x_\\star\\) is a fixed point and \\(\\abs{ f'(x_\\star)} < 1\\), then it is an attractive fixed point.\nProof. In calculus, we learn that the derivative is given by\n\\[\n\\begin{equation}\nf'(x_\\star) = \\lim\\limits_{n\\to \\infty} \\frac{f(x) -f(x_\\star)}{x-x_\\star}.\n\\end{equation}\n\\]\nThis tells us that in some neighborhood of \\(x_\\star\\), \\(f\\) is a contraction. In technical terms, there exists some \\(\\delta > 0\\) such that for \\((x_\\star - \\delta, x_\\star + \\delta)\\),\n\\[\n\\begin{equation}\n\\abs{f(x) - f(x_\\star)} < \\abs{ x-x_{\\star}}.\n\\end{equation}\n\\]\nThis means that if we pick a initial condition \\(x_0\\) that is close enough to \\(x_\\star\\), then \\(x_0\\) will get closer to \\(x_star\\) after each iteration or generation in our case.\nThis makes our problem much easier. If we want to find which \\(\\lambda\\) values have attracting fixed points, then we just check to see when \\(\\abs{ f'(x_\\star)} < 1\\). Taking the derivative of \\(f\\), we get \\(f'(x) = \\lambda(1-2x)\\). Plugging in our \\(x_\\star\\) value, we see that\n\\[\n\\begin{equation}\nf'\\left(1- \\frac{1}{\\lambda}\\right) = \\lambda\\left( 1 - 2\\left(1- \\frac{1}{\\lambda}\\right)\\right) = \\lambda\\left(\\frac{2}{\\lambda} - 1\\right) = 2 - \\lambda.\n\\end{equation}\n\\]\nThis tells us that the fixed point is attractive for \\(\\lambda\\) between 1 and 2 or between 2 and 3. Now that we’ve confirmed the existence of a fixed point and know a bit about the dynamics of this system, let’s do some simulations to see what happens to our populations over several generations.\n\n\n\n\nPopulation dynamics for different growth parameters\n\n\nAs expected from the math we did above, \\(\\lambda\\) values in \\((1,2)\\) and \\((2,3)\\) approach the fixed point \\(1 - \\frac{1}{\\lambda}\\). Relating this back to our original example, this tells us that having limited resources will cause our population to peak and stabilize. We can see this in the plot above, where \\(\\lambda_2 = 1.2\\). Looking into the rest of the cases, we can also notice that for \\(\\lambda < 1\\), the population dies out which makes sense biologically. A population that can’t successfully replenish itself is going to go extinct. What is interesting here are the values of \\(\\lambda\\) that are greater than 3. The population appears to be oscillating periodically for \\(\\lambda_4 = 3.2\\), \\(\\lambda_5 = 3.48\\), and possibly for \\(\\lambda = 3.56\\) which corresponds to the population constantly overshooting its threshold and falling back down. When we get to even higher \\(\\lambda\\) values, these population dynamics seems to fluctuate like crazy, significantly departing from the cases we looked at before."
  },
  {
    "objectID": "blog/chaos/chaos-what-is-it-where-to-find-it.html#bifurcations",
    "href": "blog/chaos/chaos-what-is-it-where-to-find-it.html#bifurcations",
    "title": "Chaos: What it is and where to find it",
    "section": "Bifurcations",
    "text": "Bifurcations\nLet’s try and visualize this behavior as \\(\\lambda\\) changes with a bifurcation diagram. A bifurcation diagram helps give us an idea about where our system will eventually end up. We take several trajectories for different \\(\\lambda\\) values and plot these \\(\\lambda\\) values against where the corresponding population ends up going in the long term. This tells us how the asymptotic behavior of our system changes alongside our growth parameter \\(\\lambda\\).\n## Values of lambda to sweep over\nlambda_range = np.linspace(2, 4, 1000)\ntail = 200\nlambd_seq = []\neq_seq = []\n\n## Run the model for 1000 generations and take at the last 200 values.\nfor lambd in lambda_range:\n    out = logistic_map(lambd = lambd, max_n = 600)\n    lambd_seq = np.append(lambd_seq, [lambd]*tail)\n    eq_seq = np.append(eq_seq, out[-tail:])\n\n\n\n\nBifurcation diagram for the logistic map\n\n\nNotice where the lines for each of our \\(\\lambda\\) values intersect. This tells us about the end behavior of the dynamics for that growth parameter value. For example, notice that \\(\\lambda_4\\) intersects the curve twice and the population dynamics seem to oscillate with a period of 2. Similarly, \\(\\lambda_5\\) intersects 4 times and we see it has period 4. We can use this idea to confirm our earlier suspicion that the dynamics for \\(\\lambda_6 = 3.56\\) are periodic. Looking at the bifurcation diagram, the line for \\(\\lambda_6\\) appears to intersect 8 times meaning the population dynamics should have period 8. This leaves us to analyze the higher values of \\(\\lambda\\). As you can see, the plot gets messy as \\(\\lambda\\) gets higher. Based on what we saw with the previous cases, there are now many, many points that your population dynamics are going to bounce between and everything becomes extremely irregular. This is chaos."
  },
  {
    "objectID": "blog/chaos/chaos-what-is-it-where-to-find-it.html#sensitivity-to-initial-conditions",
    "href": "blog/chaos/chaos-what-is-it-where-to-find-it.html#sensitivity-to-initial-conditions",
    "title": "Chaos: What it is and where to find it",
    "section": "Sensitivity to Initial Conditions",
    "text": "Sensitivity to Initial Conditions\nUp until now, our choice in initial conditions didn’t have much of an effect on where our system ended up. We can see this is the case when we’re working with \\(\\lambda\\) values less than 3. Once we begin to move our \\(\\lambda\\) values above 3, it becomes increasingly important to be precise with our choice of initial conditions \\(x_0\\). As you can see below, different but very close initial conditions \\(x_0\\) can lead to very different dynamics. This property is called sensitivity to initial conditions and is one of the hallmarks of chaos.\n\n\n\n\nVarious initial conditions for \\(\\lambda =3.95\\).\n\n\nHere, we see that the populations do diverge significantly even if they start close to one another, but this doesn’t give an idea of how quickly this occurs. Let’s take two initial conditions \\(x_0\\) and \\(x_0'\\) see how their populations change across several generations. For simplicity, let’s say that \\(x_{0} = x_{0}' + \\Delta x\\). At each generation, we’ll check to see how different the population levels are and plot this difference. Here’s the program I used to do this.\nmax_n = 40\nx_0 = 0.7\npower = -10\nDelta_x = 10**(power)\nIC = [x_0, x_0 + Delta_x]\ntime =  np.linspace(1, max_n, max_n)\n\n\nout1 = logistic_map(lambd = 3.95, max_n = max_n, x_0 = IC[0])\nout2 = logistic_map(lambd = 3.95, max_n = max_n, x_0 = IC[1])\ndist = abs(out1 - out2)\n\n\n\n\nPopulation differences for similar initial conditions.\n\n\nNotice that the difference between the populations appears to grow exponentially in time for a bit. Let’s try to get a measure of how exponential this. If we try to write the distance as \\(\\text{Dist}(t) \\approx Ce^{\\chi t}\\) for some constants \\(C\\) and \\(\\chi\\). Taking the logarithm of this expression, we can see \\[\n\\begin{equation}\n\\log(\\text{Dist}) \\approx \\chi t + \\log(C).\n\\end{equation}\n\\]\nWith it written this way, we see this is equivalent to saying \\(\\log(\\text{Dist})\\) is approximately linear in time, so we can do a linear regression to estimate the exponential rate \\(\\chi\\).\nplt.rcParams['figure.figsize'] = [12, 9]\nchi_estimate, logC, r_value, p_value, std_err = stats.linregress(time,np.log(dist))\n\nlin_reg=chi_estimate*time+logC\n\n\n\n\nLog population differences with linear regression.\n\n\nThis leaves us with \\(\\chi \\approx 0.566\\). This number \\(\\chi\\) is called a Lyapunov exponent. In more general contexts, it is used to describe how quickly extremely close trajectories diverge from one another.\nAs of now, we’ve been able to analyze the behavior of the logistic map, which can be used to describe how populations grow in conditions with limited resources. We saw that the growth parameter \\(\\lambda\\) of a population determines how it will grow over time and proved that for some \\(\\lambda\\) values the population will stabilize at a fixed value. We also showed that some growth parameters cause the population to overshoot and undershoot consistently leading to oscillation and that for even higher values we end up with chaos. We were able to visualize how this occurs. We explored what it meant to be chaotic by describing sensitivity to initial condition and getting a measure of how sensitive a system is. This was an exercise in using math to take a stab a question we might have about the real world. First, we made a crude model of how we think populations should change every generation, then we went down the rabbit hole of analyzing it. On the way, we got to see some very pretty math and make some pretty cool plots.\nThat being said, this is not the end-all-be-all of chaos. Chaos is everywhere: in population dynamics with the logistic map, in physics with things like the double pendulum, weather as in the Lorenz equations, and often in infectious disease models. They’re so many chaotic systems out in the world, and I’m itching to write some posts about others that I have or will come across. This also won’t be the last we see of the logistic map. I’m hoping to write a post about the Mandelbrot set, where it comes from, a bit on its connection to the logistic map. For now, this has been an introduction to chaos.\nFrom what we’ve seen in this introduction, chaos is not simply something changing without any rhyme or reason. It must be related to an underlying rule or process describing how things change throughout time or space for a system. Chaos is the idea that uncertainty can grow and grow exponentially quickly. Chaos is realizing that knowing where you are now doesn’t always tell you exactly where you have been in the past. It reminds us that starting from similar conditions or backgrounds doesn’t dictate that you’ll end up in the same place as one another. It tells us that all of our possible paths may at first diverge yet still possibly intersect, intertwine, and become parallel in the future. As you continue along in a chaotic system, it feels as though the future is completely unknown with no clarity to be had. This is when we’re meant to realize that the path ahead is still determined by some underlying rule, even if we don’t understand it yet.\nUntil next time,\nMarlin F."
  },
  {
    "objectID": "blog/intro-to-probability/intro-to-prob.html",
    "href": "blog/intro-to-probability/intro-to-prob.html",
    "title": "Introduction to Probability I",
    "section": "",
    "text": "Many things in life are seemingly left up to chance. In everyday life, we often hear or say phrases like “That was pretty random.” or “What are the odds of that happening?”, but can you explain what it really means for something to be ‘random’ or ‘up to chance’? Whether you’re a gambler, an anxious student, or just plain sick and tired of the weather recently, you probably want to understand something in your life that seems or feels random.\nProbability is an attempt to make sense of this perceived randomness and provide some laws or axioms for describing random processes so that we can investigate them systematically. In practice, we know that a probability describes the chance that some event will (or will not occur), but this answer alone leaves much to be desired. There’s several fundamental questions we’re left to answer.\nIn my case, the first that comes to mind is “How does one measure a probability?”"
  },
  {
    "objectID": "blog/intro-to-probability/intro-to-prob.html#a-coin-toss-is-the-most-basic-probability-model.",
    "href": "blog/intro-to-probability/intro-to-prob.html#a-coin-toss-is-the-most-basic-probability-model.",
    "title": "Introduction to Probability I",
    "section": "A coin toss is the most basic probability model.",
    "text": "A coin toss is the most basic probability model.\nIn many scenarios, we flip a coin to decide the outcome. For example, if one wants to decide which person goes first in checkers or who has to take out the trash, it’s not uncommon to hear someone say “Let’s flip a coin for it.” Why is this? Is this even fair? What does ‘fair’ even mean when you leave something up to chance like this?\nWell, you might say it’s fair because your intuition tells you that around half of the time we toss a coin it should come up heads and the other half tails. More simply said, we have two possible outcomes \\(\\omega = H\\) and \\(\\omega = T\\) which we think are equally likely (whatever that means).\nTo be precise and consistent, we’ll write these possible outcomes in terms of a set\n\\[\n\\Omega = \\{ H,T \\}.\n\\]\n\n\n\n\nVisualizing \\(\\Omega\\)\n\n\nAs you can see in the most basic coin toss, there are only two outcomes in this experiment. This is a handy model if we need to make a decision between exactly two possible choice, but what will happen if we have 3, 4, or even 1000 possible outcomes.\nLet’s switch to a more complicated example: tossing two coins sequentially. In case of two tosses, we can represent outcomes by looking at the two coin tosses separately. The first toss being \\(\\Omega_1 = \\\\{ H, T \\\\}\\) and the second \\(\\Omega_2 = \\\\{ H, T \\\\}\\). Putting these together, we can see that we have four possible outcomes\n\\[\n\\Omega = \\Omega_1 \\times \\Omega_2 = \\{ HH, HT, TH, TT\\}.\n\\]\n\n\n\n\nOutcomes \\(\\Omega\\): two coins.\n\n\nThe coin tossing example is extremely neat in that sense that it serves as a nice foundational example for the larger language around probability which we’re trying to develop. As our next step towards that goal, we’ll begin by defining a couple of terms.\nAn outcome \\(\\omega\\) is a single possible result of a given experiment like the coin tosses above.\nThe set of possible outcomes \\(\\Omega\\) is called the sample space.\nAnother important type of objects are events \\(E\\) which describe some subset of outcomes. Events enable us to ask questions about and compute probabilities concerning several different outcomes simultaneously.\nFor example, we might ask the question: “When do we have at least one heads?”. We can then look at individual outcomes \\(\\omega \\in \\Omega\\) which have at least one head and write them as a set\n\\[\nE = \\{ \\omega \\mid \\text{There is at least one $H$} \\} = \\{HH, HT, TH \\}.\n\\]\n{% include figure image_path=“/assets/images/intro-to-prob/intro-to-prob-event.jpg” height = “200” width = “100” caption=“Sample space \\(\\Omega\\) and an event \\(E\\).” %}\n\n\n\nSample space \\(\\Omega\\) and an event \\(E\\).\n\n\nOftentimes, we’re interested in the case where an event \\(E\\) does not occur. We call this event the complement of \\(E\\) and write\n\\[\nE^c = \\{w \\not\\in E \\},\n\\]\nwhich is the set of outcomes in our sample space that are not in \\(E\\). In the example above where \\(E\\) is the set where there is at least one \\(H\\),\n\\[\nE^c = \\{ \\omega \\mid \\text{There are exactly zero $H$}\\}  = \\{ TT \\}.\n\\]\nIn the case of two coin tosses, it’s easy enough to count our possible outcomes, but in order to gain a better picture of our coin-tossing experiment in general, we want to understand the specific properties or chances of our outcomes and combinations of them. That is, when discussing events like above, we want to compare and contrast them in various ways. Important tools for this are called the union (\\(\\cup\\)) and intersection (\\(\\cap\\)) of events."
  },
  {
    "objectID": "blog/intro-to-probability/intro-to-prob.html#combining-events",
    "href": "blog/intro-to-probability/intro-to-prob.html#combining-events",
    "title": "Introduction to Probability I",
    "section": "Combining events",
    "text": "Combining events\nThe union of two events \\(A\\) and \\(B\\) is simply their combination i.e. the event either \\(A\\) or \\(B\\) occurs which we write as\n\\[\nA \\cup B = \\{\\omega \\in A \\textbf{ or } \\omega \\in B \\}\n\\]\n\n\n\n\nVisualizing union (\\(\\cup\\))\n\n\nThe intersection is the overlap between two events. It is the set of outcomes that are in both \\(A\\) and \\(B\\) simultaneously.\n\\[\nA \\cap B = \\{\\omega \\in A \\textbf{ and } \\omega \\in B \\}.\n\\]\n{% include figure image_path=“/assets/images/intro-to-prob/intro-to-prob-intersection.jpg” height = “200” width = “100” caption=“Visualizing intersection (\\(\\cap\\)).” %}\n\n\n\nVisualizing intersection (\\(\\cap\\)).\n\n\nLet’s apply these ideas to the coin tossing example.\nConsider the following events. Suppose we flip two fair coins. Let \\(A\\) be the event where exactly two tosses show heads, \\(B\\) where the first coin is heads, and \\(C\\) where the second coin is tails. We begin by writing all of these different events using set notation, so that\n\\[\nA = \\{HH\\}, B = \\{HH, HT\\}, C = \\{HT, TT \\}.\n\\]\nFirst, we’ll find the union of \\(B\\) and \\(C\\). In words, this is the event where either the first coin shows heads or the second is tails. In set notation, this is simply:\n\\[\nB\\cup C = \\{HH, HT \\} \\cup \\{HT, TT \\} = \\{HH, HT, TT \\}.\n\\]\nNext up, let’s find the union of \\(A\\) and \\(B\\). This is the case where either both coins show heads or the first coin shows heads. Writing this out, we see that\n\\[\nA\\cup B = \\{HH \\} \\cup \\{HH, HT\\} = \\{HH, HT\\} = B.\n\\]\nThe event \\(A\\) is a subset of \\(B\\) (\\(A\\subset B\\)). You can think about this as follows: If both coins show heads, then the first coin obviously did. This is obvious, but it helps us make a general observation. Sometimes events will completely contain another. If every outcome in event \\(A\\) is also in event \\(B\\), we say that \\(A\\) is a subset of \\(B\\) and write \\(A\\subset B\\).\n\n\n\n\nVisualizing subsets (\\(\\subset\\)).\n\n\nLet’s try our hand with the intersection. Starting with the intersection of \\(B\\) and \\(C\\). This is the event where either the first coin shows heads and the second is tails. Using sets, we take the outcomes that \\(B\\) and \\(C\\) have in common\n\\[\nB\\cap C = \\{HH, HT \\} \\cap \\{HT, TT \\} = \\{HT \\}.\n\\]\nIn this case, our intersection only contains a single outcome \\(HT\\) i.e. the case where the first coin showed heads and the second showed tails which is exactly what we were looking for.\nOur choice of \\(B\\) and \\(C\\) lead to us having a very simple intersection, but thee fact remains that some events are disjoint or mutually exclusive, meaning that they do not share any outcomes. Take \\(A\\) and \\(C\\) for example. It is impossible for both coins to show heads and for the second coin to show tails simultaneously. We can write this as\n\\[\nA\\cap C = \\{HH \\} \\cap \\{HT, TT \\} = \\varnothing,\n\\]\nwhere \\(\\varnothing\\) is the empty set containing no elements.\nWith the above ideas in mind, how might we want to describe the probability of various events given that we know that the probability of each coin being heads is \\(\\frac{1}{2}\\)? In the next section, we’ll develop axioms for dealing with problems in probability."
  },
  {
    "objectID": "blog/intro-to-probability/intro-to-prob.html#axioms-of-probability",
    "href": "blog/intro-to-probability/intro-to-prob.html#axioms-of-probability",
    "title": "Introduction to Probability I",
    "section": "Axioms of probability",
    "text": "Axioms of probability\n\\[\n\\newcommand{ \\Prob }{ \\mathbb{P} }\n\\]\nFor this section, I’ll loosely follow Foundations of the Theory of Probability by A.N. Kolmogorov.\nIn a way, we can think of a probability as a way of measuring an event. Much like how people have weights (in both pounds and kilograms), we can think about creating a type of rule or measure \\(\\Prob\\) which weights events and collections of events relative to all possible outcomes. If we’re able to find such a measure, we want it to follow a certain set of rules much like the case of the coin tossing example.\n\n\n\n\nProbability measures \\(\\Prob\\) take sets to probabilities.\n\n\nFirst up is a simple convention, we would like all probabilities that things happen to be \\(1\\). In the simplest case, an event \\(E\\) can either surely occur so that \\(\\Prob(E) = 1\\) or it cannot \\(\\Prob(E) = 0\\). Therefore, if we look at all possible outcomes simultaneously, we have \\(\\Prob(\\Omega) = 1\\).\nSince all probabilities are positive or zero, we also require that \\(\\Prob(E)\\geq 0\\) for all events \\(E\\).\nLastly, if two events cannot occur simultaneously i.e. \\(A \\cap B = \\varnothing\\), then \\(\\Prob(A \\cup B) = \\Prob(A) + \\Prob(B).\\)\n\n\n\n\nVisualizing disjoint summations\n\n\nTherefore, our notion of probability must follow these three fundamental rules:\n\\[\n\\begin{align}\n\\Prob(\\Omega) &= 1,\\\\\n\\Prob(E) &\\geq 0 \\text{ for all events } E,\\\\\n\\Prob(A \\cup B) &= \\Prob(A) + \\Prob(B) \\text{ if } A \\cap B = \\varnothing.\n\\end{align}\n\\]\nWe call these three rules our axioms of probability. They define a way of thinking about probability that is both consistent with our intuition and mathematically useful. If you’ve seen probability before, you’ll probably notice that our three axioms are missing some of the basic statements of probability you’ve previously encountered. Most of those statements are the logical consequences of the axioms we’ve outlined. In this sense, our axioms are a way of describing probability with minimal assumptions. Namely, with these axioms alone, we can show that\n\\[\n\\Prob(A) \\leq \\Prob(B) \\text{ if } A \\subset B.\n\\]\nThis also makes sense if we return to the intuition of counting outcomes. If there are more possible outcomes in \\(B\\), then you expect that the chance \\(B\\) occurs is greater than the chance that \\(A\\) occurs as there are now more outcomes to choose from.\n\n\n\n\nProbability inequality for subsets\n\n\nWhen it comes to taking the union of subsets, there’s a more general formula for subsets which may or may not have overlap.\n\\[\n\\Prob(A \\cup B) = \\Prob(A) + \\Prob(B) - \\Prob( A \\cap B ).\n\\]\n\n\n\n\nVisualizing the general summation formation for probability of events\n\n\nFormal note: Though it is tempting to say that the set of measurable sets contains every subset, this can sometimes lead us to some strange contradictions. The idea of a permissible collection of measurable sets is formalized by a \\(\\sigma\\)-algebra. You can learn a bit more about this on wikipedia."
  },
  {
    "objectID": "blog/intro-to-probability/intro-to-prob.html#conditional-probability-and-independence",
    "href": "blog/intro-to-probability/intro-to-prob.html#conditional-probability-and-independence",
    "title": "Introduction to Probability I",
    "section": "Conditional probability and independence",
    "text": "Conditional probability and independence\nAs shown previously, the overlap between events is an important consideration when dealing with probability. Due to this overlap, we might want to ask what is the probability of an event \\(A\\) occurring if we have already observed \\(B\\). This is the notion of conditional probability. We can calculate the probability of \\(A\\) given \\(B\\) as\n\\[\n\\Prob(A\\mid B) = \\frac{\\Prob(A\\cap B)}{\\Prob(B)}.\n\\]\nIntuitively, this allows us to investigate how much additional certainty knowledge of event \\(B\\) gives us on event \\(A\\).\nIf knowing \\(B\\) gives no information on whether \\(A\\) occurred, we say that \\(A\\) and \\(B\\) are independent. Mathematically, we write that two events \\(A\\) and \\(B\\) are independent if\n\\[\n\\Prob(A\\cap B) = \\Prob(A) \\cdot \\Prob(B).\n\\]\nThis is equivalent to saying that \\(\\Prob(A \\mid B) = \\Prob(A)\\) and \\(\\Prob(B \\mid A ) = \\Prob(B)\\) which I leave to you as an exercise.\n\n\n\n\nConditional probability\n\n\nConditional probability is extremely useful because it gives a framework for understanding how certain properties of events may depend on or relate to one another.\nExercise: Prove that \\(A\\) and \\(B\\) are independent if and only if \\(\\Prob(A \\mid B) = \\Prob(A)\\) and \\(\\Prob(B \\mid A ) = \\Prob(B)\\).\nExercise: Prove the Law of Total Probability i.e. that for any two events \\(A\\) and \\(B\\):\n\\[\n\\Prob(A) = \\Prob(A \\mid B) + \\Prob(A \\mid B^c).\n\\]"
  },
  {
    "objectID": "blog/intro-to-probability/intro-to-prob.html#bayes-theorem.",
    "href": "blog/intro-to-probability/intro-to-prob.html#bayes-theorem.",
    "title": "Introduction to Probability I",
    "section": "Bayes’ theorem.",
    "text": "Bayes’ theorem.\nOne particularly important usage of conditional probabilities is Bayes’ theorem. Bayes’ theorem allows us to use the probability of \\(A\\) given \\(B\\) to calculate the probability of \\(B\\) given \\(A\\). This notion is particularly useful when it comes to analyzing how evidence in terms of data affects the likelihood of different probability distributions. Bayes’ theorem is essential for understanding many of the statistical methods used in computational biology and most computational fields which rely on Markov Chain Monte Carlo (MCMC) for fitting models to data.\nMathematically speaking, Bayes’ theorem states that\n\\[\n\\Prob(A\\mid B) = \\frac{\\Prob(B \\mid A) \\cdot \\Prob(A)}{\\Prob(B)}.\n\\]\nIn fact, the proof is simple enough that can derive this directly by writing out the conditional probabilities for \\(A\\) and \\(B\\),\n\\[\n\\begin{align}\n\\Prob(A\\mid B) &= \\frac{\\Prob(A \\cap B)}{\\Prob(B)},\\\\\n\\Prob(B\\mid A) &= \\frac{\\Prob(B \\cap A)}{\\Prob(A)}.\n\\end{align}\n\\]\nSolving for \\(\\Prob(A \\cap B) = \\Prob(B \\cap A)\\) gives us,\n\\[\n\\Prob(A \\mid B) \\cdot \\Prob(B) = \\Prob(A \\cap B) = \\Prob( B \\mid A) \\cdot   \\Prob(A)\n\\], so that\n\\[\n\\Prob(A\\mid B) = \\frac{\\Prob(B \\mid A) \\cdot \\Prob(A)}{\\Prob(B)}.\n\\]\nThis may seem like just another unmotivated equation, but Bayes’ has a useful interpretation when it comes to discrete probabilities. We’ll explore this is the following example."
  },
  {
    "objectID": "blog/intro-to-probability/intro-to-prob.html#application-vampire-hunting-with-bayes",
    "href": "blog/intro-to-probability/intro-to-prob.html#application-vampire-hunting-with-bayes",
    "title": "Introduction to Probability I",
    "section": "Application: Vampire Hunting with Bayes’",
    "text": "Application: Vampire Hunting with Bayes’\nSuppose that we’re the average American and we’re deeply concerned with discovering vampires Given that vampires are a one in a million occurrence, we can use Bayes’ theorem alongside the fact that a limited number of people are allergic to garlic and that all vampires are allergic to garlic to ‘test’ for vampires. Let’s write this in terms of probabilities. According the what we’ve written above, we know three things:\n\\[\n\\begin{align}\n\\Prob(V+) &= \\frac{1}{1000000} = 0.000001,\\\\\n\\Prob(G-\\mid V-) &= \\frac{1}{10000} = 0.00001,\\\\\n\\Prob(G- \\mid V+) &\\approx 1,\n\\end{align}\n\\]\nwhere \\(V+\\) is vampirism positive, \\(V-\\) is human, \\(G-\\) is garlic allergy, \\(G+\\) is garlic tolerant. Plugging this into Bayes’ theorem, we can calculate the probability of being a vampire if you’re allergic to garlic\n\\[\n\\Prob(V+ \\mid G-) = \\frac{\\Prob(G- \\mid V+) \\cdot \\Prob(V+)}{\\Prob(G-)}.\n\\]\nWe have all the values to compute this value except for the probability of being \\(G-\\). We can compute this by taking advantage of the conditional expectation and the Law of Total Probability. That is, we write that\n\\[\n\\Prob(G-) = \\underbrace{  \\Prob(G-\\mid V+) \\cdot \\Prob(V+) }_{\\Prob(\\text{Garlic-allergic vampires})} + \\underbrace{ \\Prob(G-\\mid V-) \\cdot \\Prob(V-) }_{\\Prob(\\text{Garlic-allergic humans})}.\n\\]\nTherefore, the probability that someone is vampire given they’re allergic to garlic\n\\[\n\\Prob(V+ \\mid G-) = \\frac{\\Prob(G- \\mid V+) \\cdot \\Prob(V+)}{ \\Prob(G-\\mid V+) \\cdot \\Prob(V+) + \\Prob(G-\\mid V-) \\cdot \\Prob(V-)}.\n\\]\nThinking of this in terms possible outcomes, this equation shows that probability of being a vampire given a known garlic allergy is the just fraction of garlic-allergic individuals who we expect to be vampires. Since garlic allergy is much more common among vampires, testing first for garlic allergies allows us to have a higher probability of correctly identifying vampires. Computing with the probabilities given above, we find that\n\\[\n\\Prob(V+ \\mid G-) \\approx 0.01 >>> \\Prob(V+) = 0.000001.\n\\]\n\n\n\n\nIllustrating Bayes\n\n\nTesting for a garlic-allergy means that we’ll be nearly 10,000 times more likely to successfully identify a vampire than if we just randomly tested or sampled the population for vampires. Though vampires are still rare among those with garlic allergies, we’ve managed to increase the probably of them being identified through the garlic test. There is still hope though. By this same logic, the better our ability to test for vampires and their unique characteristics, the easier it becomes to identify them using Bayes’ Theorem. That means that we can use other methods for identifying vampires such as mirrors and our garlic test together to further increase this probability by considering the probabilities jointly. That being said, please don’t go frolicking your neighborhood waving around garlic and hand mirrors, it’s improbable you’ll find anything.\nThis post is the first part of a series where I’ll be going over some of the basic ideas behind the basic probability and stats used in my daily research. Next up, I’ll be writing about probability distributions and estimating parameters for them. For a more thorough introduction to probability, I recommend “Theory of Probability and Random Processes” by Koralov and Sinai.\nThank you for taking the time to read! - Marlin Figgins."
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "Blog posts",
    "section": "",
    "text": "Math\n\n\n\n\nAug 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEpidemiology\n\n\n\n\nMar 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMath\n\n\n\n\nJul 12, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html",
    "href": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html",
    "title": "Social Distancing and Me",
    "section": "",
    "text": "If you’ve been glued to Twitter and/or the rest of the internet like me, you’ve been hearing a lot of talk about SARS-COV-2, social distancing and #FlattenTheCurve. We’ve arrived at the point in this pandemic where our focus has shifted from containing the virus and to slowing its progression. As of writing this, there are 154,219 confirmed cases and nearly 5,700 deaths.\nNaturally, none of us want to see others get infected and we’re all interested in slowing the spread of SARS-COV-2 but in the face of such a scary and global situation, it can be hard to visualize what role you can play in slowing the spread of SARS-COV-2 / COVID-19"
  },
  {
    "objectID": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#where-do-you-fit-into-this",
    "href": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#where-do-you-fit-into-this",
    "title": "Social Distancing and Me",
    "section": "Where do you fit into this?",
    "text": "Where do you fit into this?\nI’m going to try and show you where you may fit into this. No matter how small or large, everyone is a part of a social/contact network of some sort. Parents, friends, roommates, schoolmates, co-workers you name it. If you’re moving around in your daily life, you’re interacting with these people and almost certainly some strangers as well.\n\n\n\n\nYou, your friends, family, and other contacts.\n\n\nLet’s say that the average person has 10 people they consistently interact with in their daily life. That would mean that there’s only 10 direct ways that you can be infected and infect others, but this gets very complicated very quickly. The issue is that our lives aren’t isolated. The people in your life have friends, co-workers, and family of their own in their contact networks. For example, if they have their own 10 people, that means there are 100 people who can infect you indirectly through your contacts. Add another layer and this goes quickly to 1,000 people, 10,000 people, and on-and-on. Theoretically, you and your actions have the ability to affect hundreds of lives as transmission chains begin to grow.\n\n\n\n\nContacts of your contacts"
  },
  {
    "objectID": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#not-everyone-will-be-just-fine.",
    "href": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#not-everyone-will-be-just-fine.",
    "title": "Social Distancing and Me",
    "section": "Not everyone will be “just fine”.",
    "text": "Not everyone will be “just fine”.\nIf you’re watching the news, you’re also seeing several statistics on case fatality rates. Case fatality rates tell us the fraction of confirmed cases that are expected to die. Since only certain infected individuals are tested and confirmed as cases, it’s not quite right to say “if you become infected, you have an X percent chance of dying” because this death rate isn’t distributed evenly between all people who have the infection. As many people have been pointing out, the elderly and immunocompromised are of particularly high risk to SARS-COV-2.\n\n\n\n\nDeaths rates by age group. Source: Chinese CDC via Buzzfeed News\n\n\nEven if you don’t know someone personally who is high-risk, some of your contacts’ contacts may be at higher risk than anyone you interact with daily. They may be immunocompromised due to old age, diseases, disabilities or medications though they may appear “just fine” at a glance and have a much higher chance of having significant complication or dying if they become infected.\n\n\n\n\nFataility rates increase in the immunocompromised."
  },
  {
    "objectID": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#social-distancing-slows-infection-rates.",
    "href": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#social-distancing-slows-infection-rates.",
    "title": "Social Distancing and Me",
    "section": "Social distancing slows infection rates.",
    "text": "Social distancing slows infection rates.\nBy social distancing, you can eliminate possible transmission chains and possibly prevent someone who is at high risk from becoming sick down the line even if you personally would be fine if infected.\n{% include figure image_path=“/assets/images/social-distancing-and-me/sdae-immuno-chain.JPG” height = “200” width = “100” caption=“Possible transmission chain.” %}\n\n\n\nPossible transmission chain.\n\n\nThrough social distancing, not only do you reduce your number of contacts over time and therefore the chance of infecting someone or becoming infected but you also slow the disease’s total spread through your network.\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Transmission: Unchecked v.s. Social Distancing"
  },
  {
    "objectID": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#slowing-infection-helps-provide-healthcare-and-treatment-to-those-in-need.",
    "href": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#slowing-infection-helps-provide-healthcare-and-treatment-to-those-in-need.",
    "title": "Social Distancing and Me",
    "section": "Slowing infection helps provide healthcare and treatment to those in need.",
    "text": "Slowing infection helps provide healthcare and treatment to those in need.\nIf fewer people are infected each day, then fewer people need to be hospitalized. This allows healthcare workers to give proper time, attention, and care to those in need as they’re able to replenish their supplies and prevent people from going untreated as new cases come in. This way our local healthcare systems do not become overwhelmed as cases quickly peak. A drawn out, less severe outbreak is preferable in order for people to get access to the care they need if they’re infected. In fact, as we continue to increase the availability of treatment and slow the infection we will likely be able to drive down the mortality rate associated with COVID-19.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Health care demand: Unchecked v.s. Social Distancing"
  },
  {
    "objectID": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#take-a-step-back-flatten-the-curve.",
    "href": "blog/social-distancing-and-me/2020-03-04-social-distancing-and-me.html#take-a-step-back-flatten-the-curve.",
    "title": "Social Distancing and Me",
    "section": "Take a step back, flatten the curve.",
    "text": "Take a step back, flatten the curve.\nFlattening the curve (#FlattenTheCurve) is only possible through collective action. Our ability to slow infection and meet the healthcare demands of those in need depends on our willingness to take a step back and focus on keeping each other and especially the most vulnerable of us safe.\n\n\nAll pictures not credited were drawn by me! Thanks for reading.\nThis post was based on a twitter thread of mine embedded below:\n\n\nIf you’ve been glued to Twitter like me, you’ve been hearing a lot of talk about social distancing and #FlattenTheCurve. Naturally, we’re all interested in slowing the spread of SARS-COV-2 but sometimes its hard to visualize how exactly one person can contribute to this. 1/9\n\n— Marlin Figgins (@marlinfiggins) March 14, 2020"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Marlin Figgins",
    "section": "",
    "text": "My name is Marlin Figgins and welcome to my blog. I am currently a PhD candidate in the University of Washington’s Department of Applied Mathematics as a Boeing, ARCS, and NSF GRFP fellow.\nI currently develop Bayesian methods for inference and forecasting of epidemic and evolutionary dynamics of respiratory viruses in the Bedford lab at the Fred Hutchinson Cancer Research Center. You can learn about this work and more on the Research page.\nFor more on my professional life, you can check out my CV / Resume in PDF format on this page.\nGet in contact? Stay in contact?\nIf you need to contact me for any reason, my personal email is marlinfiggins [at] gmail [dot] com, but I can also be reached mfiggins [at] uw [dot] edu."
  }
]